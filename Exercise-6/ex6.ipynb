{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 6: Support Vector Machines\n",
    "#### Author - Rishabh Jain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Resources\n",
    "1. [SVM Video Lecture (MIT)](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "2. [29 to 33 SVM Video Lectures (University of Buffalo)](https://www.youtube.com/watch?v=N4pai7eZW_o&list=PLhuJd8bFXYJsSXPMrGlueK6TMPdHubICv&index=29)\n",
    "3. [Support Vector Machine Succinctly (PDF)](./Lectures/SVM_succinctly.pdf)\n",
    "4. [An Idiotâ€™s guide to Support vector machines](./Lectures/SVM_notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0&nbsp;&nbsp;&nbsp;&nbsp;Maths Behind SVM (Maximum Margin Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-class, such as the one shown below, there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the logistic regression find just any linear separator. **The SVM in particular defines the criterion to be looking for a decision surface that is MAXIMALLY far away from any data point**. This distance from the decision surface to the closest data point determines the margin of the classifier.\n",
    "\n",
    "<img src=\"images/svm1.PNG\" width=\"380\"/>\n",
    "\n",
    "Let's imagine a vector $\\vec{w}$ perpandicular to the margin and an unknown data point $\\vec{u}$ which can be on either side of the margin. In order to know whether $\\vec{u}$ is on the right or left side of the margin, we will project (Dot product) $\\vec{u}$ onto $\\vec{w}$.\n",
    "\n",
    "$$\\vec{w}.\\vec{u}\\geq c$$\n",
    "$$\\boxed{\\vec{w}.\\vec{u}+b\\geq 0}\\;\\;(1)$$ \n",
    "\n",
    "If the projection of $\\vec{u}$ plus some constant $b$ is greater than zero, then its a positive sample otherwise its a negative sample.**Eq. (1) is our DECISION RULE**. Here the problem is that we don't know what $w$ and $b$ to use.  \n",
    "\n",
    "**An unknown sample may be located anywhere inside or outside the margin (i.e. >0 or <0), but if it's a known positive sample $\\vec{x_{+}}$ then the SVM decision rule should insist the dot product plus some constant $b$ to be 1 or greater than 1.** Likewise for a negative sample $\\vec{x_{-}}$, dot product plus some constant $b$ should be less than or equal to -1 Hence:\n",
    "\n",
    "$\\vec{w}.\\vec{x_{+}}+b\\geq 1 $   \n",
    "$\\vec{w}.\\vec{x_{-}}+b\\leq -1 $ \n",
    "\n",
    "Introducing a variable $y_i$ such that :  \n",
    "\n",
    "$$\\begin{equation}\n",
    "  y_{i}=\\begin{cases}\n",
    "    +1 & \\text{for +ve samples}\\\\\n",
    "    -1 & \\text{for -ve samples}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "Mutiplying the above two inequality eqauations with $y_i$:\n",
    "\n",
    "For +ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$  \n",
    "For -ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$\n",
    "\n",
    "###### Note : Sign changed from $\\leq$ to $\\geq$ because $y_i$ is -1 in case of -ve samples\n",
    "Since both the equations are same, we can rewrite them as :\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)\\geq 1}\\;\\;(2)$$\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)-1= 0}\\;\\;(3)\\;\\;\\text{For samples on margin}$$\n",
    "\n",
    "Eq.(1) is basically a **constraint** for our margin, which means that **all the training samples should be on the correct side OR on the margin** (i.e. +ve samples on the right and -ve samples on the left side of the margin) and **NO training sample should be inside the margin at all meaning ZERO TRAINING ERROR.** \n",
    "\n",
    "###### Let's calculate the width of the margin.\n",
    "\n",
    "<img src=\"images/svm2.PNG\" width=\"400\"/>\n",
    "\n",
    "Let's imagine two vectors $\\vec{x_+}$ and $\\vec{x_-}$, both are +ve and -ve known samples respectively. The difference of these two vectors is a resultant vector called $\\vec{R}$ where :\n",
    "\n",
    "$$\\vec{R}=\\vec{x_+}-\\vec{x_-}$$\n",
    "\n",
    "All we need is a $\\hat{u}$, **so that the WIDTH of the margin will be the projection of $\\vec{R}$ onto $\\hat{u}$**. From the first image, we already know a vector $\\vec{w}$ in the same direction.\n",
    "\n",
    "$$\\hat{u}=\\frac{\\vec{w}}{||w||}$$\n",
    "\n",
    "**WIDTH** $=\\vec{R}.\\hat{u} $  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(\\vec{x_+}-\\vec{x_-}).\\frac{\\vec{w}}{||w||}$  \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(\\vec{x_+}.\\vec{w}-\\vec{x_-}.\\vec{w})}{||w||}$\n",
    "\n",
    "Using eq (3), we get\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(1-b+1+b)}{||w||}$\n",
    "$$\\boxed{\\text{WIDTH}=\\frac{2}{||w||}}\\;\\;(4)$$\n",
    "\n",
    "Now, we want to maximize the margin while incurring zero training error.\n",
    "\n",
    "max $\\frac{2}{||w||}$ with 0 loss OR (Flipping for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||}{2}\\;$ with 0 loss OR (Squaring the numerator for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||^2}{2}$ with 0 loss **(NO LONGER AN UNCONSTRAINED OPTIMIZATION)**\n",
    "\n",
    "##### SVM Optimization Formulation\n",
    "\n",
    "> minimize $\\;\\;\\frac{||w||^2}{2}$  \n",
    "> subject to $\\;\\;y_{i}(w^{T}x_{i}+b)\\geq 1\\;\\;,i=1,2...N$\n",
    "\n",
    "In order to solve a constrained optimization problem, Lagrange multipliers are used.  \n",
    "Note: Lagrange Multipliers are explained [**here**](#Understanding-Lagrange-Multipliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lagrange Multipliers\n",
    "Lagrange multipliers is a strategy of finding the local maxima and minima of a function subject to **equality** constraints. Let's try to solve a constrained opitimization problem :\n",
    "\n",
    "#### Example 1 (Equality Constraints) :\n",
    "\n",
    "minimize $\\;\\;f(x,y)=2-x^2-2y^2$  \n",
    "subject to $\\;\\;h(x,y)=x+y-1=0$\n",
    "\n",
    "**We introduce a new variable ($\\beta$) called a Lagrange multiplier and study the Lagrange function defined by:**\n",
    "\n",
    "$$\\boxed{L(x,y,\\beta)=f(x,y)-\\beta h(x,y)}$$\n",
    "\n",
    "$L(x,y,\\beta)=(2-x^2-2y^2)-\\beta(x+y-1)$\n",
    "\n",
    "Now we solve the above equation like an unconstrained optimization problem by taking partial derivatives w.r.t $x$ & $y$ and set them equal to zero solving for $x$, $y$ and $\\beta$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{x}}=0\\;\\;=>\\;\\;-2x-\\beta=0\\;\\;=>\\;\\;x=\\frac{-\\beta}{2}$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{y}}=0\\;\\;=>\\;\\;-4y-\\beta=0\\;\\;=>\\;\\;y=\\frac{-\\beta}{4}$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{\\beta}}=0\\;\\;=>\\;\\;x+y-1=0\\;\\;=>\\;\\;\\beta=\\frac{-4}{3}$\n",
    "\n",
    "$\\boxed{x=\\frac{4}{6},y=\\frac{4}{12},\\beta=\\frac{-4}{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 (Inequality Constraints / Karush-Kuhn-Tucker (KKT) conditions)\n",
    "\n",
    "maximize $\\;\\;f(x,y)=3x+4y$  \n",
    "subject to $\\;\\;h_{1}(x,y)=x^2+y^2\\leq4$  \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{2}(x,y)=x\\geq1$\n",
    "\n",
    "**Note: Inequality constraints should be in the form of $h(x,y)\\leq0$**\n",
    "\n",
    "$$\\boxed{L(x,y,\\alpha_1,\\alpha_2)=f(x,y)-\\alpha_1 h_{1}(x,y)-\\alpha_2 h_{2}(x,y)\\\\\\;\\;\\text{s.t. }\\alpha_1,\\alpha_2\\geq0}$$\n",
    "\n",
    "$L(x,y,\\alpha_1,\\alpha_2)=3x+4y-\\alpha_1(x^2+y^2-4)-\\alpha_2(-x+1)$  \n",
    "\n",
    "**KKT Conditions :**\n",
    "\n",
    "1. $\\frac{\\partial{L}}{\\partial{x}}=3-2\\alpha_1x+\\alpha_2=0$\n",
    "\n",
    "2. $\\frac{\\partial{L}}{\\partial{y}}=4-2\\alpha_1y=0$\n",
    "\n",
    "3. $\\alpha_1(x^2+y^2-4)=0$\n",
    "\n",
    "4. $\\alpha_2(-x+1)=0$\n",
    "\n",
    "5. $\\alpha_1,\\alpha_2\\geq0$ \n",
    "\n",
    "A constraint is considered to be binding (active) if changing it also changes the optimal solution. Less severe constraints that do not affect the optimal solution are non-binding (non active). For 2 constraints possible combinations are :\n",
    "\n",
    "- Both constraints are binding\n",
    "- Constraint 1 binding, Constraint 2 not binding\n",
    "- Constraint 2 binding, Constraing 1 not binding\n",
    "- Both constraints are not binding\n",
    "\n",
    "**POSSIBILITY 1 : Both constraints are binding**\n",
    "\n",
    "$-x+1=0\\;\\text{and}\\;\\alpha_2>0\\;\\;=>\\;\\;x=1$  \n",
    "$x^2+y^2-4=0\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x^2+y^2=4\\;\\;=>\\;\\;1+y^2=4\\;\\;=>\\;\\;y=\\pm\\sqrt{3}$  \n",
    "\n",
    "(a) For $y=+\\sqrt{3}$ \n",
    "\n",
    ">Condition 2 becomes:  \n",
    ">$4-2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{2}{\\sqrt{3}}>0$  \n",
    ">Condition 1 becomes:  \n",
    ">$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3-\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    "\n",
    "(a) For $y=-\\sqrt{3}$  \n",
    "\n",
    ">Condition 2 becomes:  \n",
    ">$4+2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{-2}{\\sqrt{3}}<0$ (KKT condition fails)    \n",
    ">Condition 1 becomes:  \n",
    ">$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3+\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{-4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    "\n",
    "**POSSIBILITY 2 : Constraint 1 binding , Contraint 2 not binding**\n",
    "\n",
    "$x>1\\;\\text{and}\\;\\boxed{\\alpha_2=0}$  \n",
    "$x^2+y^2<4\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x=+\\sqrt{4-y^{2}}$  \n",
    "\n",
    ">Condition 1 becomes:  \n",
    ">$3-2\\alpha_1x=0\\;\\;=>\\;\\;x=\\frac{3}{2\\alpha_1}\\;\\;=>\\;\\;3-2\\alpha_1\\sqrt{4-y^{2}}=0\\;\\;=>\\;\\;\\alpha_1=\\frac{3}{2\\sqrt{4-y^{2}}}$  \n",
    ">Condition 2 becomes:  \n",
    ">$4-2\\alpha_1y=0\\;\\;=>\\;\\;4-\\frac{3y}{\\sqrt{4-y^{2}}}=0\\;\\;=>\\;\\;4\\sqrt{4-y^{2}}=3y\\;\\;=>\\;\\;16(4-y^2)=9y^2\\;\\;=>\\;\\;64-16y^2=9y^2\\;\\;=>\\;\\;64=25y^2\\;\\;=>\\;\\;y=\\pm\\frac{8}{5}$\n",
    "\n",
    "$\\boxed{\\alpha_1=\\frac{3}{2\\sqrt{4-\\frac{64}{25}}}=\\frac{3}{2(\\frac{6}{5})}=\\frac{5}{4}>0}$  \n",
    "$x=+\\sqrt{4-y^{2}}\\;\\;=>\\;\\;x=\\frac{6}{5}$\n",
    "\n",
    "1 candidate point: $\\boxed{(x,y)=(\\frac{6}{5},\\frac{8}{5})}$\n",
    "\n",
    "**POSSIBILITY 3 : Constraint 2 binding , Contraint 1 not binding**\n",
    "\n",
    "$x=1\\;\\text{and}\\;\\alpha_2>0$  \n",
    "$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    "\n",
    ">Condition 2 becomes:  \n",
    ">$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    "\n",
    "**POSSIBILITY 4 : - Both constraints are not binding**\n",
    "\n",
    "$x>1\\;\\text{and}\\;\\alpha_2=0$  \n",
    "$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    "\n",
    ">Condition 2 becomes:  \n",
    ">$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    "\n",
    "**Check maximality of the candidate point :**\n",
    "\n",
    "$f(\\frac{6}{5},\\frac{8}{5})=3(\\frac{6}{4})+4(\\frac{8}{5})=\\frac{18}{5}+\\frac{32}{5}=10$\n",
    "\n",
    "Optimal Solution : $\\boxed{x=\\frac{6}{5},y=\\frac{8}{5},\\alpha_1=0,\\alpha_2=\\frac{5}{4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal and Dual Formulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
