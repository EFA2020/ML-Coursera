{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 6: Support Vector Machines\n",
    "#### Author - Rishabh Jain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Resources\n",
    "1. [SVM Video Lecture (MIT)](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "2. [29 to 33 SVM Video Lectures (University of Buffalo)](https://www.youtube.com/watch?v=N4pai7eZW_o&list=PLhuJd8bFXYJsSXPMrGlueK6TMPdHubICv&index=29)\n",
    "3. [Support Vector Machine Succinctly (PDF)](./Lectures/SVM_succinctly.pdf)\n",
    "4. [An Idiotâ€™s guide to Support vector machines](./Lectures/SVM_notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths Behind SVM (Maximum Margin Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-class, such as the one shown below, there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the logistic regression find just any linear separator. **The SVM in particular defines the criterion to be looking for a decision surface that is MAXIMALLY far away from any data point**. This distance from the decision surface to the closest data point determines the margin of the classifier.\n",
    "\n",
    "<img src=\"images/svm1.PNG\" width=\"380\"/>\n",
    "\n",
    "Let's imagine a vector $\\vec{w}$ perpendicular to the margin and an unknown data point $\\vec{u}$ which can be on either side of the margin. In order to know whether $\\vec{u}$ is on the right or left side of the margin, we will project (Dot product) $\\vec{u}$ onto $\\vec{w}$.\n",
    "\n",
    "$$\\vec{w}.\\vec{u}\\geq c$$\n",
    "$$\\boxed{\\vec{w}.\\vec{u}+b\\geq 0}\\;\\;(1)$$ \n",
    "\n",
    "If the projection of $\\vec{u}$ plus some constant $b$ is greater than zero, then its a positive sample otherwise its a negative sample.**Eq. (1) is our DECISION RULE**. Here the problem is that we don't know what $w$ and $b$ to use.  \n",
    "\n",
    "**An unknown sample may be located anywhere inside or outside the margin (i.e. >0 or <0), but if it's a known positive sample $\\vec{x_{+}}$ then the SVM decision rule should insist the dot product plus some constant $b$ to be 1 or greater than 1.** Likewise for a negative sample $\\vec{x_{-}}$, dot product plus some constant $b$ should be less than or equal to -1 Hence:\n",
    "\n",
    "$\\vec{w}.\\vec{x_{+}}+b\\geq 1 $   \n",
    "$\\vec{w}.\\vec{x_{-}}+b\\leq -1 $ \n",
    "\n",
    "Introducing a variable $y_i$ such that :  \n",
    "\n",
    "$$\\begin{equation}\n",
    "  y_{i}=\\begin{cases}\n",
    "    +1 & \\text{for +ve samples}\\\\\n",
    "    -1 & \\text{for -ve samples}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "Mutiplying the above two inequality eqauations with $y_i$:\n",
    "\n",
    "For +ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$  \n",
    "For -ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$\n",
    "\n",
    "###### Note : Sign changed from $\\leq$ to $\\geq$ because $y_i$ is -1 in case of -ve samples\n",
    "Since both the equations are same, we can rewrite them as :\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)\\geq 1}\\;\\;(2)$$\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)-1= 0}\\;\\;(3)\\;\\;\\text{For samples on margin}$$\n",
    "\n",
    "Eq.(2) is basically a **constraint** for our margin, which means that **all the training samples should be on the correct side OR on the margin** (i.e. +ve samples on the right and -ve samples on the left side of the margin) and **NO training sample should be inside the margin at all meaning ZERO TRAINING ERROR.** \n",
    "\n",
    "###### Let's calculate the width of the margin.\n",
    "\n",
    "<img src=\"images/svm2.PNG\" width=\"400\"/>\n",
    "\n",
    "Let's imagine two vectors $\\vec{x_+}$ and $\\vec{x_-}$, both are +ve and -ve known samples respectively. The difference of these two vectors is a resultant vector called $\\vec{R}$ where :\n",
    "\n",
    "$$\\vec{R}=\\vec{x_+}-\\vec{x_-}$$\n",
    "\n",
    "All we need is a $\\hat{u}$, **so that the WIDTH of the margin will be the projection of $\\vec{R}$ onto $\\hat{u}$**. From the first image, we already know a vector $\\vec{w}$ in the same direction.\n",
    "\n",
    "$$\\hat{u}=\\frac{\\vec{w}}{||w||}$$\n",
    "\n",
    "**WIDTH** $=\\vec{R}.\\hat{u} $  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(\\vec{x_+}-\\vec{x_-}).\\frac{\\vec{w}}{||w||}$  \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(\\vec{x_+}.\\vec{w}-\\vec{x_-}.\\vec{w})}{||w||}$\n",
    "\n",
    "Using eq (3), we get\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(1-b+1+b)}{||w||}$\n",
    "$$\\boxed{\\text{WIDTH}=\\frac{2}{||w||}}\\;\\;(4)$$\n",
    "\n",
    "Now, we want to maximize the margin while incurring zero training error.\n",
    "\n",
    "max $\\frac{2}{||w||}$ with 0 loss OR (Flipping for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||}{2}\\;$ with 0 loss OR (Squaring the numerator for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||^2}{2}$ with 0 loss **(NO LONGER AN UNCONSTRAINED OPTIMIZATION)**\n",
    "\n",
    "##### SVM Optimization Formulation\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{w,b}\\;\\;\\frac{||w||^2}{2}\\\\  \n",
    "\\text{subject to}\\;\\;y_{i}(w^{T}x_{i}+b)\\geq 1\\;\\;,i=1,2...N\n",
    "}\\;\\;(5)$$\n",
    "\n",
    "In order to solve a constrained optimization problem, Lagrange multipliers are used.  \n",
    ">Note: \n",
    ">* Lagrange Multipliers are explained [**here**](#Understanding-Lagrange-Multipliers).\n",
    ">* Primal and Dual formulations are explained [**here**](#Primal-and-Dual-Formulations).\n",
    "\n",
    "Since the Objective function is convex (parabola) and all the Constraints are affine (linear) too. Solving dual or primal, answer is going to be same. Rewriting above constrained optimization problem as Lagrangian:\n",
    "\n",
    "$$\\min_{w,b,\\alpha_{i}} L(w,b,\\alpha_{i})=\\frac{||w||^2}{2}+\\sum_{i=1}^{n}\\alpha_{i}(1-y_{i}(w^{T}x_{i}+b))\\\\\n",
    "\\text{subject to}\\;\\;\\alpha_{i}\\geq0,\\;\\;\\forall{i}$$\n",
    "\n",
    "Rewriting above Lagrangian function as a Dual lagrangian:\n",
    "\n",
    "$$\\max_{\\alpha_{i}} \\min_{w,b}(Lw,b,\\alpha_{i})\\\\\n",
    "\\text{subject to}\\;\\;\\alpha_{i}\\geq0,\\;\\;\\forall{i}$$\n",
    "\n",
    "**OK, let's first minimize the $L(w,b,\\alpha)$ w.r.t. $w,b$:**\n",
    "\n",
    "$$\\boxed{L(w,b,\\alpha)=\\frac{w^Tw}{2}+\\sum_{i=1}^{n}\\alpha_{i}(1-y_{i}(w^{T}x_{i}+b))}\\;\\;(6)$$\n",
    "\n",
    "$$\\min_{w,b} L(w,b,\\alpha_{i})$$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=w-\\sum_{i=1}^{n}\\alpha_{i}y_{i}x_{i}$\n",
    "\n",
    "Setting $\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=0$ gives us $\\boxed{w=\\sum_{i=1}^{n}\\alpha_{i}y_{i}x_{i}}\\;\\;(7)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=-\\sum_{i=1}^{n}\\alpha_{i}y_{i}$\n",
    "\n",
    "Setting $\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=0$ gives us $\\boxed{\\sum_{i=1}^{n}\\alpha_{i}y_{i}=0}\\;\\;(8)$\n",
    "\n",
    "Substituting eq(7) and eq(8) in eq(6),\n",
    "\n",
    "$L(w,b,\\alpha)=\\frac{w^Tw}{2}+\\sum_{i=1}^{n}\\alpha_{i}-\\sum_{i=1}^{n}\\alpha_{i}y_{i}w^{T}x_{i}-\\sum_{i=1}^{n}\\alpha_{i}y_{i}b$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_{i}y_{i}x_{i})^T(\\sum_{j=1}^{n}\\alpha_{j}y_{j}x_{j})+\\sum_{i=1}^{n}\\alpha_{i}-\\sum_{i=1}^{n}\\alpha_{i}y_{i}(\\sum_{j=1}^{n}\\alpha_{j}y_{j}x_{j})^{T}x_{i}-0$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+\\sum_{i=1}^{n}\\alpha_{i}-\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\\;\\;$\n",
    "\n",
    "**Above equation is free of any $w$ and $b$. Now, let's maximize the $L(w,b,\\alpha)$ w.r.t. $\\alpha$:**\n",
    "\n",
    "$$\\boxed{\\min_{\\alpha_{i}} L(w,b,\\alpha_{i})=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\boxed{\\alpha_{i}\\alpha_{j}}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "\\text{s.t. }\\alpha_{i}\\geq0\\\\\n",
    "\\text{and }\\sum_{i=1}^{n}\\alpha_{i}y_{i}=0}\\;\\;(9)$$\n",
    "\n",
    "Eq.(8) is Quadratic constraint optimization problem because two unknowns $\\alpha_j$ & $\\alpha_j$ are getting multiplied together. **Eq.(8) is always solved using some QP solver.**\n",
    "\n",
    "Let $\\alpha^{*}_{1},\\alpha^{*}_{2},...\\alpha^{*}_{n}$ be the solution of QP (Quadratic Programming) problem.\n",
    "\n",
    "**KKT conditions :**\n",
    "1. $\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=0,\\;\\;\\;\\;$we got Eq.(7)\n",
    "2. $\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=0,\\;\\;\\;\\;$we got Eq.(8)\n",
    "3. $y_{i}(w^{T}x_{i}+b)-1\\geq 0$\\;\\;\\;Constraints\n",
    "4. $\\alpha_{i}(y_{i}(w^TX_{i}+b)-1)=0$\n",
    "5. $\\alpha_{i}\\geq0$\n",
    "\n",
    "Using KKT condition 4 & 5, we can imply that:\n",
    "\n",
    "- If $\\alpha^{*}_{i}=0\\;\\;$ then $\\;\\;y_{i}(w^TX_{i}+b)-1\\geq0$\n",
    "- If $\\alpha^{*}_{i}>0\\;\\;$ then $\\;\\;y_{i}(w^TX_{i}+b)-1=0\\;\\;$($x$ is on the margin)\n",
    "\n",
    "**Only train examples that lie on the margin are relevant. These are called SUPPORT VECTORS.**\n",
    "\n",
    "$\\boxed{w=\\sum_{i=1}^{n}\\alpha^{*}_{i}y_{i}x_{i}}\\;\\;$ (10)\n",
    "\n",
    "For $\\alpha^{*}_{i}>0$,  \n",
    "$y_{i}(w^TX_{i}+b)-1=0\\;\\;$\n",
    "\n",
    "$\\boxed{b=\\frac{1}{y_{i}}-w^{T}x_{i}}\\;\\;$ (11)\n",
    "\n",
    "All $b$'s are ideally equal otherwise an average is taken.\n",
    "\n",
    "**Now for a test point $x^{*}$ :** \n",
    "\n",
    "$y=w^Tx^*+b$  \n",
    "\n",
    "$\\;\\;\\;=(\\sum_{i=0}^{n}\\alpha_{i}^{*}y_{i}x_{i})^{T}x^{*}+b$  \n",
    "\n",
    "$\\;\\;\\;=\\sum_{\\alpha_{i}>0}^{n}\\alpha_{i}^{*}y_{i}(x_{i}^{T}x^{*})+b\\;\\;$ Only true for Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the data is not linearly separable??\n",
    "\n",
    "#### Soft Margin SVM Classifier\n",
    "\n",
    ">- Cannot go for **Zero training error**\n",
    ">- Still learn a maximum margin hyperplane\n",
    ">>- Allow some examples to be misclassified\n",
    ">>- Allow some examples to fall inside the margin\n",
    ">- Cutting some slack during training\n",
    "><img src=\"images/svm3.PNG\" width=\"380\"/>\n",
    ">- **Separable Case:** To ensure zero training loss, constraint was\n",
    ">\n",
    ">$$y_{i}(w^{T}x_{i}+b)\\geq1\\;\\;\\;\\forall_{i}=1...N$$\n",
    ">\n",
    ">- **Non-separable Case:** Relax the constraint by introducing slack ($\\xi$)\n",
    ">\n",
    ">$$y_{i}(w^{T}x_{i}+b)\\geq1-\\xi_{i}\\;\\;\\;\\forall_{i}=1...N$$\n",
    ">\n",
    ">- $\\xi_{i}$ is called **Slack variable** ($\\xi_{i}\\geq0$).\n",
    ">- For misclassification, $\\xi_{i}>1$\n",
    ">- It is OK to have some misclassified training examples\n",
    ">>- Some $\\xi_{i}$'s will be non-zero.\n",
    ">- Minimize the number of such examples (Ensuring not too many points on the wrong side of margin)\n",
    ">>- Minimize $\\sum_{i=1}^N\\xi_{i}$\n",
    ">- **Optimization Problem for Non-Separable case where C controls the impact of the margin and the margin error**.\n",
    ">\n",
    ">$$\\boxed{\n",
    "\\max_{w,b}f(w,b)=\\frac{||w||^{2}}{2}+C\\sum_{i=1}^N\\xi_{i}\\\\\n",
    "\\text{subject to}\\;\\;\\;y_{i}(w^{T}x_{i}+b)\\geq1-\\xi_{i},\\;\\;\\;\\xi_{i}\\geq0\\;i=1,...N\n",
    "}\\;\\;(12)$$\n",
    "\n",
    "#### Kernel Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lagrange Multipliers\n",
    "Lagrange multipliers is a strategy of finding the local maxima and minima of a function subject to **equality** constraints. Let's try to solve a constrained opitimization problem :\n",
    "\n",
    "#### Example 1 (Equality Constraint)\n",
    "\n",
    ">minimize $\\;\\;f(x,y)=2-x^2-2y^2$  \n",
    ">subject to $\\;\\;h(x,y)=x+y-1=0$\n",
    ">\n",
    ">**We introduce a new variable ($\\beta$) called a Lagrange multiplier and study the Lagrange function defined by:**\n",
    ">\n",
    ">$$\\boxed{L(x,y,\\beta)=f(x,y)-\\beta h(x,y)}$$\n",
    ">\n",
    ">$L(x,y,\\beta)=(2-x^2-2y^2)-\\beta(x+y-1)$\n",
    ">\n",
    ">Now we solve the above equation like an unconstrained optimization problem by taking partial derivatives w.r.t $x$ & $y$ and set them equal to zero solving for $x$, $y$ and $\\beta$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{x}}=0\\;\\;=>\\;\\;-2x-\\beta=0\\;\\;=>\\;\\;x=\\frac{-\\beta}{2}$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{y}}=0\\;\\;=>\\;\\;-4y-\\beta=0\\;\\;=>\\;\\;y=\\frac{-\\beta}{4}$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{\\beta}}=0\\;\\;=>\\;\\;x+y-1=0\\;\\;=>\\;\\;\\beta=\\frac{-4}{3}$\n",
    ">\n",
    ">$\\boxed{x=\\frac{4}{6},y=\\frac{4}{12},\\beta=\\frac{-4}{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 (Inequality Constraints / Karush-Kuhn-Tucker (KKT) conditions)\n",
    "\n",
    ">maximize $\\;\\;f(x,y)=3x+4y$  \n",
    ">subject to $\\;\\;h_{1}(x,y)=x^2+y^2\\leq4$  \n",
    ">$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{2}(x,y)=x\\geq1$\n",
    ">\n",
    ">**Note: Inequality constraints should be in the form of $h(x,y)\\leq0$**\n",
    ">\n",
    ">$$\\boxed{L(x,y,\\alpha_1,\\alpha_2)=f(x,y)-\\alpha_1 h_{1}(x,y)-\\alpha_2 h_{2}(x,y)\\\\\\;\\;\\text{s.t. }\\alpha_1,\\alpha_2\\geq0}$$\n",
    ">\n",
    ">$L(x,y,\\alpha_1,\\alpha_2)=3x+4y-\\alpha_1(x^2+y^2-4)-\\alpha_2(-x+1)$  \n",
    ">\n",
    ">**KKT Conditions :**\n",
    ">\n",
    ">1. $\\frac{\\partial{L}}{\\partial{x}}=3-2\\alpha_1x+\\alpha_2=0$\n",
    ">\n",
    ">2. $\\frac{\\partial{L}}{\\partial{y}}=4-2\\alpha_1y=0$\n",
    ">\n",
    ">3. $\\alpha_1(x^2+y^2-4)=0$\n",
    ">\n",
    ">4. $\\alpha_2(-x+1)=0$\n",
    ">\n",
    ">5. $\\alpha_1,\\alpha_2\\geq0$ \n",
    ">\n",
    ">A constraint is considered to be binding (active) if changing it also changes the optimal solution. Less severe constraints that do not affect the optimal solution are non-binding (non active). For 2 constraints possible combinations are :\n",
    ">\n",
    ">- Both constraints are binding\n",
    ">- Constraint 1 binding, Constraint 2 not binding\n",
    ">- Constraint 2 binding, Constraing 1 not binding\n",
    ">- Both constraints are not binding\n",
    ">\n",
    ">**POSSIBILITY 1 : Both constraints are binding**\n",
    ">\n",
    ">$-x+1=0\\;\\text{and}\\;\\alpha_2>0\\;\\;=>\\;\\;x=1$  \n",
    ">$x^2+y^2-4=0\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x^2+y^2=4\\;\\;=>\\;\\;1+y^2=4\\;\\;=>\\;\\;y=\\pm\\sqrt{3}$  \n",
    ">\n",
    ">(a) For $y=+\\sqrt{3}$ \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{2}{\\sqrt{3}}>0$  \n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3-\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    ">\n",
    ">(a) For $y=-\\sqrt{3}$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4+2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{-2}{\\sqrt{3}}<0$ (KKT condition fails)    \n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3+\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{-4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    ">\n",
    ">**POSSIBILITY 2 : Constraint 1 binding , Contraint 2 not binding**\n",
    ">\n",
    ">$x>1\\;\\text{and}\\;\\boxed{\\alpha_2=0}$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x=+\\sqrt{4-y^{2}}$  \n",
    ">\n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1x=0\\;\\;=>\\;\\;x=\\frac{3}{2\\alpha_1}\\;\\;=>\\;\\;3-2\\alpha_1\\sqrt{4-y^{2}}=0\\;\\;=>\\;\\;\\alpha_1=\\frac{3}{2\\sqrt{4-y^{2}}}$  \n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4-\\frac{3y}{\\sqrt{4-y^{2}}}=0\\;\\;=>\\;\\;4\\sqrt{4-y^{2}}=3y\\;\\;=>\\;\\;16(4-y^2)=9y^2\\;\\;=>\\;\\;64-16y^2=9y^2\\;\\;=>\\;\\;64=25y^2\\;\\;=>\\;\\;y=\\pm\\frac{8}{5}$\n",
    ">\n",
    ">$\\boxed{\\alpha_1=\\frac{3}{2\\sqrt{4-\\frac{64}{25}}}=\\frac{3}{2(\\frac{6}{5})}=\\frac{5}{4}>0}$  \n",
    ">$x=+\\sqrt{4-y^{2}}\\;\\;=>\\;\\;x=\\frac{6}{5}$\n",
    ">\n",
    ">1 candidate point: $\\boxed{(x,y)=(\\frac{6}{5},\\frac{8}{5})}$\n",
    ">\n",
    ">**POSSIBILITY 3 : Constraint 2 binding , Contraint 1 not binding**\n",
    ">\n",
    ">$x=1\\;\\text{and}\\;\\alpha_2>0$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    ">\n",
    ">**POSSIBILITY 4 : Both constraints are not binding**\n",
    ">\n",
    ">$x>1\\;\\text{and}\\;\\alpha_2=0$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    ">\n",
    ">**Check maximality of the candidate point :**\n",
    ">\n",
    ">$f(\\frac{6}{5},\\frac{8}{5})=3(\\frac{6}{4})+4(\\frac{8}{5})=\\frac{18}{5}+\\frac{32}{5}=10$\n",
    ">\n",
    ">Optimal Solution : $\\boxed{x=\\frac{6}{5},y=\\frac{8}{5},\\alpha_1=0,\\alpha_2=\\frac{5}{4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling both types of Constraints\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{w}\\;\\;f(w)\\\\\n",
    "\\text{subject to}\\;\\;g_{i}(w)\\leq0\\;\\;\\;i=1,2,...k\\\\\n",
    "\\text{and}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{i}(w)=0\\;\\;\\;i=1,2,...l\\\\\n",
    "}$$\n",
    "\n",
    "**Generalized Lagrangian**  \n",
    "$$\\boxed{\n",
    "L(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^{k}\\alpha_{i}g_{i}(w)+\\sum_{i=1}^{l}\\beta_{i}h_{i}(w)\\\\\n",
    "\\text{subject to}\\;\\;\\alpha_{i}\\geq0,\\forall_i\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal and Dual Formulations\n",
    "\n",
    "#### Primal Optimization\n",
    "\n",
    ">Let $\\theta_p$ be defined as :\n",
    ">$$\\boxed{\\theta_p(w)=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}L(w,\\alpha,\\beta)}$$\n",
    ">\n",
    ">Original constrained problem is same as :\n",
    ">$$\\boxed{p^*=\\min_{w}\\theta_P(w)=\\min_{w}\\max_{\\alpha,\\beta;\\alpha_i\\geq0}L(w,\\alpha,\\beta)}$$\n",
    ">\n",
    ">Solving $p^*$ is same as solving the constrained optimization problem.\n",
    ">\n",
    ">$$\\begin{equation}\n",
    ">  \\theta_{p}(w)=\\begin{cases}\n",
    ">    f(w) & \\text{if all constraints are satifsied}\\\\\n",
    ">    \\infty & \\text{else}\n",
    ">  \\end{cases}\n",
    ">\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual Optimization\n",
    ">Let $\\theta_d$ be defined as :\n",
    ">$$\\boxed{\\theta_d(w)=\\min_{w}L(w,\\alpha,\\beta)}$$\n",
    ">\n",
    ">Original constrained problem is same as :\n",
    ">$$\\boxed{d^*=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}\\theta_d(w)=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}\\min_{w}L(w,\\alpha,\\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week Duality Theorem (Min-Max Theorem)\n",
    "\n",
    ">$$\\boxed{d^{*}\\leq p^{*}}$$\n",
    ">\n",
    ">Both of them are equal ($d^{*}=p^{*}$) when\n",
    ">- f(w) is convex\n",
    ">- Constraints are affine (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation between Primal and Dual\n",
    "\n",
    "+ In genral $d^*\\leq p^*$, for SVM optimization the equality holds true.\n",
    "+ Certain conditions should be true.\n",
    "+ Known as the **Kahrun-Kuhn-Tucker (KKT)** conditions.\n",
    "+ For $d^*=p^*=L(w^*,\\alpha^*,\\beta^*)$ :\n",
    ">+ $\\frac{\\partial}{\\partial{w}}L(w^*,\\alpha^*,\\beta^*)=0$   \n",
    ">+ $\\frac{\\partial}{\\partial{\\beta_{i}}}L(w^*,\\alpha^*,\\beta^*)=0\\;\\;\\;\\;\\;\\;\\;i=1,2,...l$  \n",
    ">+ $\\alpha_{i}g_{i}(w^{*})=0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$  \n",
    ">+ $g_{i}(w^{*})\\leq0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$  \n",
    ">+ $\\alpha_{i}\\geq0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
