{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 6: Support Vector Machines\n",
    "#### Author - Rishabh Jain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Resources\n",
    "1. [SVM Video Lecture (MIT)](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "2. [29 to 33 SVM Video Lectures (University of Buffalo)](https://www.youtube.com/watch?v=N4pai7eZW_o&list=PLhuJd8bFXYJsSXPMrGlueK6TMPdHubICv&index=29)\n",
    "3. [Support Vector Machine Succinctly (PDF)](./Lectures/SVM_succinctly.pdf)\n",
    "4. [An Idiotâ€™s guide to Support vector machines](./Lectures/SVM_notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0&nbsp;&nbsp;&nbsp;&nbsp;Maths Behind SVM (Maximum Margin Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-class, such as the one shown below, there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the logistic regression find just any linear separator. **The SVM in particular defines the criterion to be looking for a decision surface that is MAXIMALLY far away from any data point**. This distance from the decision surface to the closest data point determines the margin of the classifier.\n",
    "\n",
    "<img src=\"./images/svm1.png\" width=\"380\">\n",
    "\n",
    "Let's imagine a vector $\\vec{w}$ perpandicular to the margin and an unknown data point $\\vec{u}$ which can be on either side of the margin. In order to know whether $\\vec{u}$ is on the right or left side of the margin, we will project (Dot product) $\\vec{u}$ onto $\\vec{w}$.\n",
    "\n",
    "$$\\vec{w}.\\vec{u}\\geq c$$\n",
    "$$\\boxed{\\vec{w}.\\vec{u}+b\\geq 0}\\;\\;(1)$$ \n",
    "\n",
    "If the projection of $\\vec{u}$ plus some constant $b$ is greater than zero, then its a positive sample otherwise its a negative sample.**Eq. (1) is our DECISION RULE**. Here the problem is that we don't know what $w$ and $b$ to use.  \n",
    "\n",
    "**An unknown sample may be located anywhere inside or outside the margin (i.e. >0 or <0), but if it's a known positive sample $\\vec{x_{+}}$ then the SVM decision rule should insist the dot product plus some constant $b$ to be 1 or greater than 1.** Likewise for a negative sample $\\vec{x_{-}}$, dot product plus some constant $b$ should be less than or equal to -1 Hence:\n",
    "\n",
    "$\\vec{w}.\\vec{x_{+}}+b\\geq 1 $   \n",
    "$\\vec{w}.\\vec{x_{-}}+b\\leq -1 $ \n",
    "\n",
    "Introducing a variable $y_i$ such that :  \n",
    "\n",
    "$$\\begin{equation}\n",
    "  y_{i}=\\begin{cases}\n",
    "    +1 & \\text{for +ve samples}\\\\\n",
    "    -1 & \\text{for -ve samples}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "Mutiplying the above two inequality eqauations with $y_i$:\n",
    "\n",
    "For +ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$  \n",
    "For -ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$\n",
    "\n",
    "###### Note : Sign changed from $\\leq$ to $\\geq$ because $y_i$ is -1 in case of -ve samples\n",
    "Since both the equations are same, we can rewrite them as :\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)\\geq 1}\\;\\;(2)$$\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)-1= 0}\\;\\;(3)\\;\\;\\text{For samples on margin}$$\n",
    "\n",
    "Eq.(1) is basically a **constraint** for our margin, which means that **all the training samples should be on the correct side OR on the margin** (i.e. +ve samples on the right and -ve samples on the left side of the margin) and **NO training sample should be inside the margin at all meaning ZERO TRAINING ERROR.** \n",
    "\n",
    "###### Let's calculate the width of the margin.\n",
    "\n",
    "<img src=\"./images/svm2.png\" width=\"400\">\n",
    "\n",
    "Let's imagine two vectors $\\vec{x_+}$ and $\\vec{x_-}$, both are +ve and -ve known samples respectively. The difference of these two vectors is a resultant vector called $\\vec{R}$ where :\n",
    "\n",
    "$$\\vec{R}=\\vec{x_+}-\\vec{x_-}$$\n",
    "\n",
    "All we need is a $\\hat{u}$, **so that the WIDTH of the margin will be the projection of $\\vec{R}$ onto $\\hat{u}$**. From the first image, we already know a vector $\\vec{w}$ in the same direction.\n",
    "\n",
    "$$\\hat{u}=\\frac{\\vec{w}}{||w||}$$\n",
    "\n",
    "**WIDTH** $=\\vec{R}.\\hat{u} $  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(\\vec{x_+}-\\vec{x_-}).\\frac{\\vec{w}}{||w||}$  \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(\\vec{x_+}.\\vec{w}-\\vec{x_-}.\\vec{w})}{||w||}$\n",
    "\n",
    "Using eq (3), we get\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(1-b+1+b)}{||w||}$\n",
    "$$\\boxed{\\text{WIDTH}=\\frac{2}{||w||}}\\;\\;(4)$$\n",
    "\n",
    "Now, we want to maximize the margin while incurring zero training error.\n",
    "\n",
    "max $\\frac{2}{||w||}$ with 0 loss OR (Flipping for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||}{2}\\;$ with 0 loss OR (Squaring the numerator for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||^2}{2}$ with 0 loss **(NO LONGER A CONSTRAINED OPTIMIZATION)**\n",
    "\n",
    "##### Optimization Formulation\n",
    "\n",
    "> minimize $\\;\\;\\frac{||w||^2}{2}$  \n",
    "> subject to $\\;\\;y_{i}(\\vec{w}.\\vec{x_{i}}+b)\\geq 1\\;\\;,i=1,2...N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
