{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 6: Support Vector Machines\n",
    "#### Author - Rishabh Jain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Resources\n",
    "1. [SVM Video Lecture (MIT)](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "2. [29 to 33 SVM Video Lectures (University of Buffalo)](https://www.youtube.com/watch?v=N4pai7eZW_o&list=PLhuJd8bFXYJsSXPMrGlueK6TMPdHubICv&index=29)\n",
    "3. [Support Vector Machine Succinctly (PDF)](./Lectures/SVM_succinctly.pdf)\n",
    "4. [An Idiotâ€™s guide to Support vector machines](./Lectures/SVM_notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths Behind SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-class, such as the one shown below, there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the logistic regression find just any linear separator. **The SVM in particular defines the criterion to be looking for a decision surface that is MAXIMALLY far away from any data point**. This distance from the decision surface to the closest data point determines the margin of the classifier.\n",
    "\n",
    "<img src=\"images/svm1.PNG\" width=\"380\"/>\n",
    "\n",
    "Let's imagine a vector $\\vec{w}$ perpendicular to the margin and an unknown data point $\\vec{u}$ which can be on either side of the margin. In order to know whether $\\vec{u}$ is on the right or left side of the margin, we will project (Dot product) $\\vec{u}$ onto $\\vec{w}$.\n",
    "\n",
    "$$\\vec{w}.\\vec{u}\\geq c$$\n",
    "$$\\boxed{\\vec{w}.\\vec{u}+b\\geq 0}\\;\\;(1)$$ \n",
    "\n",
    "If the projection of $\\vec{u}$ plus some constant $b$ is greater than zero, then its a positive sample otherwise its a negative sample.**Eq. (1) is our DECISION RULE**. Here the problem is that we don't know what $w$ and $b$ to use.  \n",
    "\n",
    "**An unknown sample may be located anywhere inside or outside the margin (i.e. >0 or <0), but if it's a known positive sample $\\vec{x_{+}}$ then the SVM decision rule should insist the dot product plus some constant $b$ to be 1 or greater than 1.** Likewise for a negative sample $\\vec{x_{-}}$, dot product plus some constant $b$ should be less than or equal to -1 Hence:\n",
    "\n",
    "$\\vec{w}.\\vec{x_{+}}+b\\geq 1 $   \n",
    "$\\vec{w}.\\vec{x_{-}}+b\\leq -1 $ \n",
    "\n",
    "Introducing a variable $y_i$ such that :  \n",
    "\n",
    "$$\\begin{equation}\n",
    "  y_{i}=\\begin{cases}\n",
    "    +1 & \\text{for +ve samples}\\\\\n",
    "    -1 & \\text{for -ve samples}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "Mutiplying the above two inequality eqauations with $y_i$:\n",
    "\n",
    "For +ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$  \n",
    "For -ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$\n",
    "\n",
    "###### Note : Sign changed from $\\leq$ to $\\geq$ because $y_i$ is -1 in case of -ve samples\n",
    "Since both the equations are same, we can rewrite them as :\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)\\geq 1}\\;\\;(2)$$\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)-1= 0}\\;\\;(3)\\;\\;\\text{For samples on margin}$$\n",
    "\n",
    "Eq.(2) is basically a **constraint** for our margin, which means that **all the training samples should be on the correct side OR on the margin** (i.e. +ve samples on the right and -ve samples on the left side of the margin) and **NO training sample should be inside the margin at all meaning ZERO TRAINING ERROR.** \n",
    "\n",
    "###### Let's calculate the width of the margin.\n",
    "\n",
    "<img src=\"images/svm2.PNG\" width=\"400\"/>\n",
    "\n",
    "Let's imagine two vectors $\\vec{x_+}$ and $\\vec{x_-}$, both are +ve and -ve known samples respectively. The difference of these two vectors is a resultant vector called $\\vec{R}$ where :  \n",
    "$$\\vec{R}=\\vec{x_+}-\\vec{x_-}$$\n",
    "\n",
    "All we need is a $\\hat{u}$, **so that the WIDTH of the margin will be the projection of $\\vec{R}$ onto $\\hat{u}$**. From the first image, we already know a vector $\\vec{w}$ in the same direction.  \n",
    "$$\\hat{u}=\\frac{\\vec{w}}{||w||}$$  \n",
    "**WIDTH** $=\\vec{R}.\\hat{u} $  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(\\vec{x_+}-\\vec{x_-}).\\frac{\\vec{w}}{||w||}$  \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(\\vec{x_+}.\\vec{w}-\\vec{x_-}.\\vec{w})}{||w||}$\n",
    "\n",
    "Using Eq.(3), we get\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(1-b+1+b)}{||w||}$\n",
    "$$\\boxed{\\text{WIDTH}=\\frac{2}{||w||}}\\;\\;(4)$$\n",
    "\n",
    "Now, we want to maximize the margin while incurring zero training error.\n",
    "\n",
    "max $\\frac{2}{||w||}$ with 0 loss OR (Flipping for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||}{2}\\;$ with 0 loss OR (Squaring the numerator for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||^2}{2}$ with 0 loss **(NO LONGER AN UNCONSTRAINED OPTIMIZATION)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: \n",
    ">* Lagrange Multipliers are explained [**here**](#Understanding-Lagrange-Multipliers).\n",
    ">* Primal and Dual formulations are explained [**here**](#Primal-and-Dual-Formulations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-Margin SVM Classifier\n",
    "\n",
    "#### Optimization Formulation\n",
    "$$\\boxed{\n",
    "\\min_{w,b}\\;\\;\\frac{||w||^2}{2}\\\\  \n",
    "\\text{s.t.}\\;\\;y_{i}(w^{T}x_{i}+b)\\geq 1,\\;\\;\\forall{i}\n",
    "}\\;\\;(5)$$\n",
    "\n",
    "In order to solve a constrained optimization problem, Lagrange multipliers are used.  \n",
    "\n",
    "Since the Objective function is convex (parabola) and all the Constraints are affine (linear) too. Solving dual or primal, answer is going to be same. Rewriting above constrained optimization problem as Lagrangian:\n",
    "\n",
    "$$\\boxed{\\min_{w,b,\\alpha} L(w,b,\\alpha)=\\frac{||w||^2}{2}+\\sum_{i=1}^{N}\\alpha_{i}(1-y_{i}(w^{T}x_{i}+b))\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\;\\;\\forall{i}}\\;\\;(6)$$\n",
    "\n",
    "Rewriting above Lagrangian function as a Dual lagrangian:\n",
    "\n",
    "$$\\max_{\\alpha} \\min_{w,b}(Lw,b,\\alpha)\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\;\\;\\forall{i}$$\n",
    "\n",
    "**OK, let's first minimize the $L(w,b,\\alpha)$ w.r.t. $w,b$:**\n",
    "\n",
    "$$\\min_{w,b} L(w,b,\\alpha)$$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=w-\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=0$ gives us $\\boxed{w=\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}}\\;\\;(7)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=-\\sum_{i=1}^{N}\\alpha_{i}y_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=0$ gives us $\\boxed{\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0}\\;\\;(8)$\n",
    "\n",
    "We now will take Eq.(7) and Eq.(8) and plug them back into our full lagrangian Eq.(6) to get a reduced lagrangian that depends only on $\\alpha$:\n",
    "\n",
    "$\\begin{align*}\n",
    "L(w,b,\\alpha)&=\\frac{w^Tw}{2}+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}y_{i}w^{T}x_{i}-\\sum_{i=1}^{N}\\alpha_{i}y_{i}b\\\\\n",
    "&=\\frac{1}{2}(\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})^T(\\sum_{j=1}^{N}\\alpha_{j}y_{j}x_{j})+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}y_{i}(\\sum_{j=1}^{N}\\alpha_{j}y_{j}x_{j})^{T}x_{i}-0\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "&=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\n",
    "\\end{align*}$\n",
    "\n",
    "**Above equation is free of any $w$ and $b$. Now, let's maximize the $L(w,b,\\alpha)$ w.r.t. $\\alpha$:**\n",
    "\n",
    "$$\\boxed{\\max_{\\alpha} L(w,b,\\alpha)=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\boxed{\\alpha_{i}\\alpha_{j}}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "\\text{s.t. }\\alpha_{i}\\geq0\\;\\;\\forall{i}\\\\\n",
    "\\text{and }\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\\;\\;\\forall{i}}\\;\\;(9)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft-Margin SVM Classifier\n",
    "\n",
    "#### What if the data is not linearly separable??\n",
    "If the data is not linearly separable, Hard-Margin SVM will fail to fit the data as it tries to incur zero training error. Here, incurring zero training error is not possible but we can still learn a maximum margin hyperplane if we:\n",
    "\n",
    "<img src=\"images/svm3.PNG\" width=\"380\"/>\n",
    "\n",
    "- Relax the constraint by introducing **slack variable** $(\\xi_{i})$\n",
    "- Allow some examples to be **misclassified**. For misclassification, $(\\xi_{i}>1)$\n",
    "- Allow some examples to fall inside the margin\n",
    "- Minimize the number of such examples (Ensuring not too many points are on the wrong side of margin)\n",
    "$$\\min C\\sum_{i=1}^N\\xi_{i}$$ \n",
    "Where **C controls the impact of the margin and the margin error.**\n",
    "\n",
    "#### Optimization Formulation\n",
    "$$\\boxed{\n",
    "\\min_{w,b}f(w,b)=\\frac{||w||^{2}}{2}+C\\sum_{i=1}^{N}\\xi_{i}\\\\\n",
    "\\text{s.t.}\\;\\;\\;y_{i}(w^{T}x_{i}+b)\\geq1-\\xi_{i},\\;\\;\\forall{i}\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\xi_{i}\\geq0,\\;\\;\\forall{i}\n",
    "}\\;\\;(12)$$\n",
    "\n",
    "Rewriting above constrained optimization problem as Lagrangian:\n",
    "\n",
    "$$\\boxed{\\min_{w,b,\\xi,\\alpha,\\beta} L(w,b,\\xi,\\alpha,\\beta)=\\frac{||w||^2}{2}+C\\sum_{i=1}^{N}\\xi{i}+\\sum_{i=1}^{N}\\alpha_{i}[(1-\\xi_{i})-y_{i}(w^{T}x_{i}+b)]+\\sum_{i=1}^{N}\\beta_{i}(-\\xi_{i})\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\beta_{i}\\geq0,\\;\\;\\forall{i}}\\;\\;(13)$$\n",
    "\n",
    "Rewriting above Lagrangian function as a Dual lagrangian:\n",
    "\n",
    "$$\\max_{\\alpha\\beta} \\min_{w,b,\\xi}L(w,b,\\xi)\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\beta_{i}\\geq0,\\;\\;\\forall{i}$$\n",
    "\n",
    "**OK, let's first minimize the $L(w,b,\\xi,\\alpha)$ w.r.t. $w,b$ and $\\xi$ :**\n",
    "\n",
    "$$\\min_{w,b,\\xi} L(w,b,\\xi,\\alpha,\\beta)$$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{w}}L(w,b,\\xi,\\alpha,\\beta)=w-\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{w}}L(w,b,\\xi,\\alpha,\\beta)=0$ gives us $\\boxed{w=\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}}\\;\\;\\text{Same as Eq.(7)}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{b}}L(w,b,\\xi,\\alpha,\\beta)=-\\sum_{i=1}^{N}\\alpha_{i}y_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{b}}L(w,b,\\xi,\\alpha,\\beta)=0$ gives us $\\boxed{\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0}\\;\\;\\text{Same as Eq.(8)}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{\\xi}}L(w,b,\\xi,\\alpha,\\beta)=C-\\alpha_{i}-\\beta_{i}$\n",
    "\n",
    ">$\\begin{align*}\n",
    "\\text{Setting }&\\frac{\\partial}{\\partial{\\xi}}L(w,b,\\xi,\\alpha,\\beta)=0\\\\\n",
    "&=>\\boxed{C-\\alpha_{i}-\\beta_{i}=0}\\;\\;(14)\\\\\n",
    "&=>\\beta_{i}=C-\\alpha_{i}\\\\\\\\\n",
    "&\\text{But because the }\\beta_{i}\\text{'s are a dual variables with }\\beta_{i}\\geq0\\text{, then this leads to:}\\\\\n",
    "&=>C-\\alpha_{i}\\geq0\\\\\n",
    "&=>\\alpha_{i}\\leq C\\\\\\\\\n",
    "&\\text{This along with the fact that }\\alpha_{i}\\text{ are dual variables with }\\alpha_{i}\\geq0\\text{ we have:}\\\\\n",
    "&\\boxed{0\\leq\\alpha_{i}\\leq C}\\;\\;(15)\n",
    "\\end{align*}$\n",
    "\n",
    "We now will take these results and plug them back into our full lagrangian to get a reduced lagrangian that depends only on $\\alpha$ and $\\beta$:\n",
    "\n",
    "Replacing $w$ using Eq.(7) in Eq.(13), we get\n",
    "\n",
    "$\\begin{align*}\n",
    "L(w,b,\\xi,\\alpha,\\beta)&=\\frac{1}{2}(\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})^{T}(\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}[(1-\\xi_{i})-y_{i}((\\sum_{j=1}^{N}\\alpha_{j}y_{j}x_{j})^{T}x_{i}+b)]-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}[(1-\\xi_{i})-(\\sum_{j=1}^{N}\\alpha_{j}y_{i}y_{j}x_{j})^{T}x_{i}+b]-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}\\xi_{i}-\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}+b\\sum_{i=1}^{N}\\alpha_{i}y_{i}-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}\\xi_{i}+b\\sum_{i=1}^{N}\\alpha_{i}y_{i}-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=-\\frac{1}{2}\n",
    "\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-(\\sum_{i=1}^{N}\\alpha_{i}+\\sum_{i=1}^{N}\\beta_{i})\\xi_{i}\\;\\;\\text{Using Eq.(8)}\\\\\n",
    "&=-\\frac{1}{2}\n",
    "\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-C\\sum_{i=1}^{N}\\xi_{i}\\;\\;\\text{Using Eq.(14)}\\\\\n",
    "&=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\n",
    "\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\n",
    "\\end{align*}$\n",
    "\n",
    "**Above equation is free of any $w,b,\\xi$ and $\\beta$. Now, let's maximize the $L(w,b,\\xi,\\alpha,\\beta)$ w.r.t. $\\alpha$:**\n",
    "\n",
    "$$\\boxed{\\max_{\\alpha} L(w,b,\\alpha)=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\boxed{\\alpha_{i}\\alpha_{j}}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "\\text{s.t. }0\\leq\\alpha_{i}\\leq C\\;\\;\\forall{i}\\\\\n",
    "\\text{and }\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\\;\\;\\forall{i}}\\;\\;(16)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KKT conditions (Hard & Soft SVM)\n",
    "\n",
    "Eq.(9) & Eq.(16) is Quadratic constraint optimization problem because two unknowns $\\alpha_j$ & $\\alpha_j$ are getting multiplied together. **Eq.(9) is always solved using some QP solver (like cvxopt in python).**\n",
    "\n",
    "Let $\\alpha^{*}_{1},\\alpha^{*}_{2},...\\alpha^{*}_{N}$ be the solution of QP (Quadratic Programming) problem.\n",
    "\n",
    "1. $\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=0,\\;\\;\\;\\;$we got Eq.(7)\n",
    "2. $\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=0,\\;\\;\\;\\;$we got Eq.(8)\n",
    "3. $y_{i}(w^{T}x_{i}+b)-1\\geq 0\\;\\;\\;$Constraints\n",
    "4. $\\alpha_{i}(y_{i}(w^TX_{i}+b)-1)=0$\n",
    "5. $\\alpha_{i}\\geq0$\n",
    "\n",
    "Using KKT condition 4 & 5, we can imply that:\n",
    "\n",
    "- If $\\alpha^{*}_{i}=0\\;\\;$ then $\\;\\;y_{i}(w^TX_{i}+b)-1\\geq0$\n",
    "- If $\\alpha^{*}_{i}>0\\;\\;$ then $\\;\\;y_{i}(w^TX_{i}+b)-1=0\\;\\;$($x$ is on the margin)\n",
    "\n",
    "**Only train examples that lie on the margin are relevant. These are called SUPPORT VECTORS.**\n",
    "\n",
    "$$\\boxed{w=\\sum_{i=1}^{N}\\alpha^{*}_{i}y_{i}x_{i}}\\;\\;(10)$$ \n",
    "\n",
    "For $\\alpha^{*}_{i}>0$,  \n",
    "$y_{i}(w^TX_{i}+b)-1=0\\;\\;$\n",
    "\n",
    "$$\\boxed{b=\\frac{1}{y_{i}}-w^{T}x_{i}}\\;\\;(11)$$\n",
    "\n",
    "All $b$'s are ideally equal otherwise an average is taken.\n",
    "\n",
    "**Now for a test point $x^{*}$ :** \n",
    "\n",
    "$\\begin{align*}\n",
    "y&=w^Tx^*+b\\\\\n",
    "&=(\\sum_{i=0}^{N}\\alpha_{i}^{*}y_{i}x_{i})^{T}x^{*}+b\\\\  \n",
    "&=\\sum_{\\alpha_{i}>0}^{N}\\alpha_{i}^{*}y_{i}(x_{i}^{T}x^{*})+b\\;\\;\\text{(Only true for Support Vectors)}\\\\\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for SVM\n",
    "\n",
    "If $w^{T}x+b>1$ then $\\xi=0$  \n",
    "else $\\xi=1-(w^{T}x+b)$\n",
    "\n",
    "**Hinge Loss** : $\\boxed{J=\\sum_{i=1}^{n}max(0,1-y_{i}(w^{T}x+b))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Implementation using CVXOPT Solver\n",
    "\n",
    "#### Disclaimer\n",
    "This SVM implementation using CVXOPT is only for educational purpose and will not help you win any MNIST competitions. The sole purpose of this implementation is to apply the mathematics explained above and understand the effect of various kernels.\n",
    "\n",
    "#### Rewriting the SVM Optimzation problem in CVXOPT format\n",
    "Since we will solve the optimization problem using CVXOPT library in python, we will need to match the solver's API which, according to the documentation is of the form:\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{x} \\frac{1}{2}x^{T}Px+q^{T}x\\\\\n",
    "s.t.\\;\\;Gx\\leq h\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;Ax=b\n",
    "}$$\n",
    "\n",
    "With api:  \n",
    ">`cvxopt.solvers.qp(P, q[, G, h[, A, b[, solver[, initvals]]]])`\n",
    "\n",
    "Let H be a matrix such that $\\boxed{H_{i,j}=\\sum_{i=1}^{N}\\sum_{j=1}^{N}y_{i}y_{j}(x_{i}x_{j})}$, then Eq.(9) becomes:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\max_{\\alpha}&\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\alpha^{T}H\\alpha\\\\\n",
    "s.t.\\\\\n",
    "&\\alpha_{i}\\geq0\\\\\n",
    "&\\sum_{i=0}^{N}\\alpha_{i}y_{i}=0\n",
    "\\end{align*}$$\n",
    "\n",
    "We convert the sums into vector form and multiply both the objective and the constraint by -1 which turns this into a minimization problem and reverses the inequality :\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{\\alpha}\\frac{1}{2}\\alpha^{T}H\\alpha-1^{T}\\alpha\\\\\n",
    "\\text{s.t.}\\;\\;-\\alpha\\leq0\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;y^{T}\\alpha=0\n",
    "}$$\n",
    "\n",
    "We are now ready to convert our numpy arrays into the CVXOPT format, using the same notation as in the documentation this gives:\n",
    "\n",
    "- P $:=$ H (a matrix of size $m$ x $m$)\n",
    "- q $:=$ $\\vec{-1}$ (a vector of size $m$ x $1$)\n",
    "- G $:=$ -diag(1) (a diagonal matrix of -1s of size $m$ x $m$)\n",
    "- h $:=$ $\\vec{0}$ (a vector of zeros of size $m$ x $1$)\n",
    "- A $:=$ $y^{T}$ (the label vector of size $1$ x $m$)\n",
    "- b $:=$ 0 (a scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernels:\n",
    "    \n",
    "    @classmethod\n",
    "    def linear(cls,x1,x2):\n",
    "        return np.dot(x1,x2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self,kernel=Kernels.linear,C=None):\n",
    "        self.kernel=kernel\n",
    "        self.C=C\n",
    "        if self.C is not None:\n",
    "            self.C=float(self.c)\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        # Gram Matrix\n",
    "        K=np.zeros(shape=(m,m))\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                K[i,j]=self.kernel(X[i],X[j])\n",
    "        H=np.multiply(np.outer(y,y),K)\n",
    "        P=cvxopt.matrix(H)\n",
    "        q=cvxopt.matrix(np.ones(m)*-1)\n",
    "        # For Hard Margin\n",
    "        if self.C is None:\n",
    "            G=cvxopt.matrix(np.diag(np.ones(m)*-1))\n",
    "            h=cvxopt.matrix(np.zeros((m,1)))\n",
    "        # For Soft Margin\n",
    "        else:\n",
    "            pass\n",
    "        A=cvxopt.matrix(y.astype(float),(1,m))\n",
    "        b=cvxopt.matrix(0.0)\n",
    "        # Solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6952e-01 -3.8466e-01  4e+00  2e+00  1e+00\n",
      " 1: -2.0068e-01 -2.2979e-01  6e-01  3e-01  2e-01\n",
      " 2:  1.4264e-02 -3.9847e-03  1e-01  3e-02  2e-02\n",
      " 3: -7.5013e-04 -1.3848e-06  4e-03  1e-03  7e-04\n",
      " 4: -7.5107e-06 -1.3907e-10  4e-05  1e-05  7e-06\n",
      " 5: -7.5108e-08 -1.3907e-14  4e-07  1e-07  7e-08\n",
      " 6: -7.5108e-10 -1.3907e-18  4e-09  1e-09  7e-10\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[1,2,3,4],[3,4,5,6],[1,2,4,5]])\n",
    "y=np.array([1,2,4])\n",
    "\n",
    "svm=SVM()\n",
    "svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1&nbsp;&nbsp;&nbsp;&nbsp;Support Vector Machines\n",
    "\n",
    "In the first half of the exercise, we will be using SVMs with various examples 2D datasets. And in the next half of the exercise, we will be using SVM to build a spam classifier.\n",
    "\n",
    "#### 1.1&nbsp;&nbsp;&nbsp;&nbsp;Example Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply((np.ones((5,5))),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lagrange Multipliers\n",
    "Lagrange multipliers is a strategy of finding the local maxima and minima of a function subject to **equality** constraints. Let's try to solve a constrained opitimization problem :\n",
    "\n",
    "#### Example 1 (Equality Constraint)\n",
    "\n",
    ">minimize $\\;\\;f(x,y)=2-x^2-2y^2$  \n",
    ">subject to $\\;\\;h(x,y)=x+y-1=0$\n",
    ">\n",
    ">**We introduce a new variable ($\\beta$) called a Lagrange multiplier and study the Lagrange function defined by:**\n",
    ">\n",
    ">$$\\boxed{L(x,y,\\beta)=f(x,y)-\\beta h(x,y)}$$\n",
    ">\n",
    ">$L(x,y,\\beta)=(2-x^2-2y^2)-\\beta(x+y-1)$\n",
    ">\n",
    ">Now we solve the above equation like an unconstrained optimization problem by taking partial derivatives w.r.t $x$ & $y$ and set them equal to zero solving for $x$, $y$ and $\\beta$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{x}}=0\\;\\;=>\\;\\;-2x-\\beta=0\\;\\;=>\\;\\;x=\\frac{-\\beta}{2}$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{y}}=0\\;\\;=>\\;\\;-4y-\\beta=0\\;\\;=>\\;\\;y=\\frac{-\\beta}{4}$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{\\beta}}=0\\;\\;=>\\;\\;x+y-1=0\\;\\;=>\\;\\;\\beta=\\frac{-4}{3}$\n",
    ">\n",
    ">$\\boxed{x=\\frac{4}{6},y=\\frac{4}{12},\\beta=\\frac{-4}{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 (Inequality Constraints / Karush-Kuhn-Tucker (KKT) conditions)\n",
    "\n",
    ">maximize $\\;\\;f(x,y)=3x+4y$  \n",
    ">subject to $\\;\\;h_{1}(x,y)=x^2+y^2\\leq4$  \n",
    ">$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{2}(x,y)=x\\geq1$\n",
    ">\n",
    ">**Note: Inequality constraints should be in the form of $h(x,y)\\leq0$**\n",
    ">\n",
    ">$$\\boxed{L(x,y,\\alpha_1,\\alpha_2)=f(x,y)-\\alpha_1 h_{1}(x,y)-\\alpha_2 h_{2}(x,y)\\\\\\;\\;\\text{s.t. }\\alpha_1,\\alpha_2\\geq0}$$\n",
    ">\n",
    ">$L(x,y,\\alpha_1,\\alpha_2)=3x+4y-\\alpha_1(x^2+y^2-4)-\\alpha_2(-x+1)$  \n",
    ">\n",
    ">**KKT Conditions :**\n",
    ">\n",
    ">1. $\\frac{\\partial{L}}{\\partial{x}}=3-2\\alpha_1x+\\alpha_2=0$\n",
    ">\n",
    ">2. $\\frac{\\partial{L}}{\\partial{y}}=4-2\\alpha_1y=0$\n",
    ">\n",
    ">3. $\\alpha_1(x^2+y^2-4)=0$\n",
    ">\n",
    ">4. $\\alpha_2(-x+1)=0$\n",
    ">\n",
    ">5. $\\alpha_1,\\alpha_2\\geq0$ \n",
    ">\n",
    ">A constraint is considered to be binding (active) if changing it also changes the optimal solution. Less severe constraints that do not affect the optimal solution are non-binding (non active). For 2 constraints possible combinations are :\n",
    ">\n",
    ">- Both constraints are binding\n",
    ">- Constraint 1 binding, Constraint 2 not binding\n",
    ">- Constraint 2 binding, Constraing 1 not binding\n",
    ">- Both constraints are not binding\n",
    ">\n",
    ">**POSSIBILITY 1 : Both constraints are binding**\n",
    ">\n",
    ">$-x+1=0\\;\\text{and}\\;\\alpha_2>0\\;\\;=>\\;\\;x=1$  \n",
    ">$x^2+y^2-4=0\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x^2+y^2=4\\;\\;=>\\;\\;1+y^2=4\\;\\;=>\\;\\;y=\\pm\\sqrt{3}$  \n",
    ">\n",
    ">(a) For $y=+\\sqrt{3}$ \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{2}{\\sqrt{3}}>0$  \n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3-\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    ">\n",
    ">(a) For $y=-\\sqrt{3}$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4+2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{-2}{\\sqrt{3}}<0$ (KKT condition fails)    \n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3+\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{-4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    ">\n",
    ">**POSSIBILITY 2 : Constraint 1 binding , Contraint 2 not binding**\n",
    ">\n",
    ">$x>1\\;\\text{and}\\;\\boxed{\\alpha_2=0}$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x=+\\sqrt{4-y^{2}}$  \n",
    ">\n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1x=0\\;\\;=>\\;\\;x=\\frac{3}{2\\alpha_1}\\;\\;=>\\;\\;3-2\\alpha_1\\sqrt{4-y^{2}}=0\\;\\;=>\\;\\;\\alpha_1=\\frac{3}{2\\sqrt{4-y^{2}}}$  \n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4-\\frac{3y}{\\sqrt{4-y^{2}}}=0\\;\\;=>\\;\\;4\\sqrt{4-y^{2}}=3y\\;\\;=>\\;\\;16(4-y^2)=9y^2\\;\\;=>\\;\\;64-16y^2=9y^2\\;\\;=>\\;\\;64=25y^2\\;\\;=>\\;\\;y=\\pm\\frac{8}{5}$\n",
    ">\n",
    ">$\\boxed{\\alpha_1=\\frac{3}{2\\sqrt{4-\\frac{64}{25}}}=\\frac{3}{2(\\frac{6}{5})}=\\frac{5}{4}>0}$  \n",
    ">$x=+\\sqrt{4-y^{2}}\\;\\;=>\\;\\;x=\\frac{6}{5}$\n",
    ">\n",
    ">1 candidate point: $\\boxed{(x,y)=(\\frac{6}{5},\\frac{8}{5})}$\n",
    ">\n",
    ">**POSSIBILITY 3 : Constraint 2 binding , Contraint 1 not binding**\n",
    ">\n",
    ">$x=1\\;\\text{and}\\;\\alpha_2>0$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    ">\n",
    ">**POSSIBILITY 4 : Both constraints are not binding**\n",
    ">\n",
    ">$x>1\\;\\text{and}\\;\\alpha_2=0$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    ">\n",
    ">**Check maximality of the candidate point :**\n",
    ">\n",
    ">$f(\\frac{6}{5},\\frac{8}{5})=3(\\frac{6}{4})+4(\\frac{8}{5})=\\frac{18}{5}+\\frac{32}{5}=10$\n",
    ">\n",
    ">Optimal Solution : $\\boxed{x=\\frac{6}{5},y=\\frac{8}{5},\\alpha_1=0,\\alpha_2=\\frac{5}{4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling both types of Constraints\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{w}\\;\\;f(w)\\\\\n",
    "\\text{subject to}\\;\\;g_{i}(w)\\leq0\\;\\;\\;i=1,2,...k\\\\\n",
    "\\text{and}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{i}(w)=0\\;\\;\\;i=1,2,...l\\\\\n",
    "}$$\n",
    "\n",
    "**Generalized Lagrangian**  \n",
    "$$\\boxed{\n",
    "L(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^{k}\\alpha_{i}g_{i}(w)+\\sum_{i=1}^{l}\\beta_{i}h_{i}(w)\\\\\n",
    "\\text{subject to}\\;\\;\\alpha_{i}\\geq0,\\forall_i\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal and Dual Formulations\n",
    "\n",
    "#### Primal Optimization\n",
    "\n",
    "Let $\\theta_p$ be defined as :\n",
    "$$\\boxed{\\theta_p(w)=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}L(w,\\alpha,\\beta)}$$\n",
    "\n",
    "Original constrained problem is same as :\n",
    "$$\\boxed{p^*=\\min_{w}\\theta_P(w)=\\min_{w}\\max_{\\alpha,\\beta;\\alpha_i\\geq0}L(w,\\alpha,\\beta)}$$\n",
    "\n",
    "Solving $p^*$ is same as solving the constrained optimization problem.\n",
    "\n",
    "$$\\begin{equation}\n",
    "  \\theta_{p}(w)=\\begin{cases}\n",
    "    f(w) & \\text{if all constraints are satifsied}\\\\\n",
    "    \\infty & \\text{else}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual Optimization\n",
    "Let $\\theta_d$ be defined as :\n",
    "$$\\boxed{\\theta_d(w)=\\min_{w}L(w,\\alpha,\\beta)}$$\n",
    "\n",
    "Original constrained problem is same as :\n",
    "$$\\boxed{d^*=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}\\theta_d(w)=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}\\min_{w}L(w,\\alpha,\\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week Duality Theorem (Min-Max Theorem)\n",
    "\n",
    "$$\\boxed{d^{*}\\leq p^{*}}$$\n",
    "\n",
    "Both of them are equal ($d^{*}=p^{*}$) when\n",
    "- f(w) is convex\n",
    "- Constraints are affine (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation between Primal and Dual\n",
    "\n",
    "+ In genral $d^*\\leq p^*$, for SVM optimization the equality holds true.\n",
    "+ Certain conditions should be true.\n",
    "+ Known as the **Kahrun-Kuhn-Tucker (KKT)** conditions.\n",
    "+ For $d^*=p^*=L(w^*,\\alpha^*,\\beta^*)$ :\n",
    ">+ $\\frac{\\partial}{\\partial{w}}L(w^*,\\alpha^*,\\beta^*)=0$   \n",
    ">+ $\\frac{\\partial}{\\partial{\\beta_{i}}}L(w^*,\\alpha^*,\\beta^*)=0\\;\\;\\;\\;\\;\\;\\;i=1,2,...l$  \n",
    ">+ $\\alpha_{i}g_{i}(w^{*})=0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$  \n",
    ">+ $g_{i}(w^{*})\\leq0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$  \n",
    ">+ $\\alpha_{i}\\geq0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
