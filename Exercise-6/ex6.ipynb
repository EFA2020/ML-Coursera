{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 6: Support Vector Machines\n",
    "#### Author - Rishabh Jain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Resources\n",
    "1. [SVM Video Lecture (MIT)](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "2. [29 to 33 SVM Video Lectures (University of Buffalo)](https://www.youtube.com/watch?v=N4pai7eZW_o&list=PLhuJd8bFXYJsSXPMrGlueK6TMPdHubICv&index=29)\n",
    "3. [Support Vector Machine Succinctly (PDF)](./Lectures/svm.pdf)\n",
    "\n",
    "\n",
    "Before solving the programming assignment from the course, let's try to understand the **Maths Behind SVM :**\n",
    "1. [Maximum Margin Classifier](#Maximum-Margin-Classifier)\n",
    "2. [Lagrange Multipliers](#Understanding-Lagrange-Multipliers)\n",
    "3. [Primal and Dual Lagrangian](#Primal-and-Dual-Formulations)\n",
    "4. [Hard Margin SVM](#Hard-Margin-SVM)\n",
    "5. [Soft Margin SVM](#Soft-Margin-SVM)\n",
    "6. [SVM Kernel Trick](#SVM-Kernel-Trick)\n",
    "7. [SVM Implementation using CVXOPT](#SVM-Implementation-using-CVXOPT-Solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-class, such as the one shown below, there are lots of possible linear separators. Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes. While some learning methods such as the logistic regression find just any linear separator. **The SVM in particular defines the criterion to be looking for a decision surface that is MAXIMALLY far away from any data point**. This distance from the decision surface to the closest data point determines the margin of the classifier.\n",
    "\n",
    "<img src=\"images/svm1.PNG\" width=\"380\"/>\n",
    "\n",
    "Let's imagine a vector $\\vec{w}$ perpendicular to the margin and an unknown data point $\\vec{u}$ which can be on either side of the margin. In order to know whether $\\vec{u}$ is on the right or left side of the margin, we will project (Dot product) $\\vec{u}$ onto $\\vec{w}$.\n",
    "\n",
    "$$\\vec{w}.\\vec{u}\\geq c$$\n",
    "$$\\boxed{\\vec{w}.\\vec{u}+b\\geq 0}\\;\\;(1)$$ \n",
    "\n",
    "If the projection of $\\vec{u}$ plus some constant $b$ is greater than zero, then its a positive sample otherwise its a negative sample.**Eq. (1) is our DECISION RULE**. Here the problem is that we don't know what $w$ and $b$ to use.  \n",
    "\n",
    "**An unknown sample may be located anywhere inside or outside the margin (i.e. >0 or <0), but if it's a known positive sample $\\vec{x_{+}}$ then the SVM decision rule should insist the dot product plus some constant $b$ to be 1 or greater than 1.** Likewise for a negative sample $\\vec{x_{-}}$, dot product plus some constant $b$ should be less than or equal to -1 Hence:\n",
    "\n",
    "$\\vec{w}.\\vec{x_{+}}+b\\geq 1 $   \n",
    "$\\vec{w}.\\vec{x_{-}}+b\\leq -1 $ \n",
    "\n",
    "Introducing a variable $y_i$ such that :  \n",
    "\n",
    "$$\\begin{equation}\n",
    "  y_{i}=\\begin{cases}\n",
    "    +1 & \\text{for +ve samples}\\\\\n",
    "    -1 & \\text{for -ve samples}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "Mutiplying the above two inequality eqauations with $y_i$:\n",
    "\n",
    "For +ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$  \n",
    "For -ve sample : $y_{i}(\\vec{w}.\\vec{x_{i}}+b) \\geq 1$\n",
    "\n",
    "###### Note : Sign changed from $\\leq$ to $\\geq$ because $y_i$ is -1 in case of -ve samples\n",
    "Since both the equations are same, we can rewrite them as :\n",
    "\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)\\geq 1}\\;\\;(2)$$\n",
    "For samples on margin\n",
    "$$\\boxed{y_{i}(\\vec{w}.\\vec{x_{i}}+b)-1= 0}\\;\\;(3)$$\n",
    "\n",
    "Eq.(2) is basically a **constraint** for our margin, which means that **all the training samples should be on the correct side OR on the margin** (i.e. +ve samples on the right and -ve samples on the left side of the margin) and **NO training sample should be inside the margin at all meaning ZERO TRAINING ERROR.** \n",
    "\n",
    "###### Let's calculate the width of the margin.\n",
    "\n",
    "<img src=\"images/svm2.PNG\" width=\"400\"/>\n",
    "\n",
    "Let's imagine two vectors $\\vec{x_+}$ and $\\vec{x_-}$, both are +ve and -ve known samples respectively. The difference of these two vectors is a resultant vector called $\\vec{R}$ where :  \n",
    "$$\\vec{R}=\\vec{x_+}-\\vec{x_-}$$\n",
    "\n",
    "All we need is a $\\hat{u}$, **so that the WIDTH of the margin will be the projection of $\\vec{R}$ onto $\\hat{u}$**. From the first image, we already know a vector $\\vec{w}$ in the same direction.  \n",
    "$$\\hat{u}=\\frac{\\vec{w}}{||w||}$$  \n",
    "**WIDTH** $=\\vec{R}.\\hat{u} $  \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=(\\vec{x_+}-\\vec{x_-}).\\frac{\\vec{w}}{||w||}$  \n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(\\vec{x_+}.\\vec{w}-\\vec{x_-}.\\vec{w})}{||w||}$\n",
    "\n",
    "Using Eq.(3), we get\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\\frac{(1-b+1+b)}{||w||}$\n",
    "$$\\boxed{\\text{WIDTH}=\\frac{2}{||w||}}\\;\\;(4)$$\n",
    "\n",
    "Now, we want to maximize the margin while incurring zero training error.\n",
    "\n",
    "max $\\frac{2}{||w||}$ with 0 loss OR (Flipping for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||}{2}\\;$ with 0 loss OR (Squaring the numerator for mathematical convenience)\n",
    "\n",
    "min $\\frac{||w||^2}{2}$ with 0 loss **(NO LONGER AN UNCONSTRAINED OPTIMIZATION)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-Margin SVM\n",
    "\n",
    "#### Optimization Formulation\n",
    "$$\\boxed{\n",
    "\\min_{w,b}\\;\\;\\frac{||w||^2}{2}\\\\  \n",
    "\\text{s.t.}\\;\\;y_{i}(w^{T}x_{i}+b)\\geq 1,\\;\\;\\forall{i}\n",
    "}\\;\\;(5)$$\n",
    "\n",
    "In order to solve a constrained optimization problem, Lagrange multipliers are used.  \n",
    "\n",
    "Since the Objective function is convex (parabola) and all the Constraints are affine (linear) too. Solving dual or primal, answer is going to be same. Rewriting above constrained optimization problem as Lagrangian:\n",
    "\n",
    "$$\\boxed{\\min_{w,b,\\alpha} L(w,b,\\alpha)=\\frac{||w||^2}{2}+\\sum_{i=1}^{N}\\alpha_{i}(1-y_{i}(w^{T}x_{i}+b))\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\;\\;\\forall{i}}\\;\\;(6)$$\n",
    "\n",
    "Rewriting above Lagrangian function as a Dual lagrangian:\n",
    "\n",
    "$$\\max_{\\alpha} \\min_{w,b}(Lw,b,\\alpha)\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\;\\;\\forall{i}$$\n",
    "\n",
    "**OK, let's first minimize the $L(w,b,\\alpha)$ w.r.t. $w,b$:**\n",
    "\n",
    "$$\\min_{w,b} L(w,b,\\alpha)$$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=w-\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=0$ gives us $\\boxed{w=\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}}\\;\\;(7)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=-\\sum_{i=1}^{N}\\alpha_{i}y_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=0$ gives us $\\boxed{\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0}\\;\\;(8)$\n",
    "\n",
    "We now will take Eq.(7) and Eq.(8) and plug them back into our full lagrangian Eq.(6) to get a reduced lagrangian that depends only on $\\alpha$:\n",
    "\n",
    "$\\begin{align*}\n",
    "L(w,b,\\alpha)&=\\frac{w^Tw}{2}+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}y_{i}w^{T}x_{i}-\\sum_{i=1}^{N}\\alpha_{i}y_{i}b\\\\\n",
    "&=\\frac{1}{2}(\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})^T(\\sum_{j=1}^{N}\\alpha_{j}y_{j}x_{j})+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}y_{i}(\\sum_{j=1}^{N}\\alpha_{j}y_{j}x_{j})^{T}x_{i}-0\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "&=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\n",
    "\\end{align*}$\n",
    "\n",
    "**Above equation is free of any $w$ and $b$. Now, let's maximize the $L(w,b,\\alpha)$ w.r.t. $\\alpha$:**\n",
    "\n",
    "$$\\boxed{\\max_{\\alpha} L(w,b,\\alpha)=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\boxed{\\alpha_{i}\\alpha_{j}}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "\\text{s.t. }\\alpha_{i}\\geq0\\;\\;\\forall{i}\\\\\n",
    "\\text{and }\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\\;\\;\\forall{i}}\\;\\;(9)$$\n",
    "\n",
    "#### KKT Conditions\n",
    "\n",
    "Eq.(9) is a Quadratic constraint optimization problem because two unknowns $\\alpha_j$ & $\\alpha_j$ are getting multiplied together. **So, this equation is solved using some QP solver (like CVXOPT in python).**\n",
    "\n",
    "Let $\\alpha^{*}_{1},\\alpha^{*}_{2},...\\alpha^{*}_{N}$ be the solutions of QP (Quadratic Programming) problem.\n",
    "\n",
    "**Stationary conition:**\n",
    "1. $\\frac{\\partial}{\\partial{w}}L(w,b,\\alpha)=0,\\;\\;\\;\\;$we got Eq.(7)\n",
    "2. $\\frac{\\partial}{\\partial{b}}L(w,b,\\alpha)=0,\\;\\;\\;\\;$we got Eq.(8)\n",
    "\n",
    "**Primal feasibility condition:**\n",
    "3. $y_{i}(w^{T}x_{i}+b)-1\\geq 0\\;\\;\\;$\n",
    "\n",
    "**Dual feasibility condition:**\n",
    "4. $\\alpha_{i}\\geq0$\n",
    "\n",
    "**Complementary slackness condition:**\n",
    "5. $\\alpha_{i}(y_{i}(w^TX_{i}+b)-1)=0$\n",
    "\n",
    "Using KKT condition 4 & 5, we can imply that:\n",
    "\n",
    "- If $\\alpha^{*}_{i}=0\\;\\;$ then $\\;\\;y_{i}(w^TX_{i}+b)-1\\geq0$\n",
    "- If $\\alpha^{*}_{i}>0\\;\\;$ then $\\;\\;y_{i}(w^TX_{i}+b)-1=0\\;\\;$($x$ is on the margin)\n",
    "\n",
    "**Only train examples that lie on the margin are relevant. These are called SUPPORT VECTORS.**\n",
    "\n",
    "$$\\boxed{w=\\sum_{i=1}^{N}\\alpha^{*}_{i}y_{i}x_{i}}\\;\\;(10)$$ \n",
    "\n",
    "For $\\alpha^{*}_{i}>0$,  \n",
    "$y_{i}(w^TX_{i}+b)-1=0\\;\\;$\n",
    "\n",
    "$$\\boxed{b=\\frac{1}{y_{i}}-w^{T}x_{i}}\\;\\;(11)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft-Margin SVM\n",
    "\n",
    "#### What if the data is not linearly separable??\n",
    "If the data is not linearly separable, Hard-Margin SVM will fail to fit the data as it tries to incur zero training error. Here, incurring zero training error is not possible but we can still learn a maximum margin hyperplane if we:\n",
    "\n",
    "<img src=\"images/svm3.PNG\" width=\"380\"/>\n",
    "\n",
    "- Relax the constraint by introducing **slack variable** $(\\xi_{i})$\n",
    "- Allow some examples to fall inside the margin, $(0<\\xi_{i}\\leq1)$\n",
    "- Allow some examples to be **misclassified**. For misclassification, $(\\xi_{i}>1)$\n",
    "- Minimize the number of such examples (Ensuring not too many points are on the wrong side of margin)\n",
    "$$\\min C\\sum_{i=1}^N\\xi_{i}$$ \n",
    "Where **C controls the impact of the margin error.**\n",
    "\n",
    "#### Optimization Formulation\n",
    "$$\\boxed{\n",
    "\\min_{w,b}f(w,b)=\\frac{||w||^{2}}{2}+C\\sum_{i=1}^{N}\\xi_{i}\\\\\n",
    "\\text{s.t.}\\;\\;\\;y_{i}(w^{T}x_{i}+b)\\geq1-\\boxed{\\xi_{i}},\\;\\;\\forall{i}\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\xi_{i}\\geq0,\\;\\;\\forall{i}\n",
    "}\\;\\;(12)$$\n",
    "\n",
    "Rewriting above constrained optimization problem as Lagrangian:\n",
    "\n",
    "$$\\boxed{\\min_{w,b,\\xi,\\alpha,\\beta} L(w,b,\\xi,\\alpha,\\beta)=\\frac{||w||^2}{2}+C\\sum_{i=1}^{N}\\xi{i}+\\sum_{i=1}^{N}\\alpha_{i}[(1-\\xi_{i})-y_{i}(w^{T}x_{i}+b)]+\\sum_{i=1}^{N}\\beta_{i}(-\\xi_{i})\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\beta_{i}\\geq0,\\;\\;\\forall{i}}\\;\\;(13)$$\n",
    "\n",
    "Rewriting above Lagrangian function as a Dual lagrangian:\n",
    "\n",
    "$$\\max_{\\alpha\\beta} \\min_{w,b,\\xi}L(w,b,\\xi)\\\\\n",
    "\\text{s.t.}\\;\\;\\alpha_{i}\\geq0,\\beta_{i}\\geq0,\\;\\;\\forall{i}$$\n",
    "\n",
    "**OK, let's first minimize the $L(w,b,\\xi,\\alpha)$ w.r.t. $w,b$ and $\\xi$ :**\n",
    "\n",
    "$$\\min_{w,b,\\xi} L(w,b,\\xi,\\alpha,\\beta)$$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{w}}L(w,b,\\xi,\\alpha,\\beta)=w-\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{w}}L(w,b,\\xi,\\alpha,\\beta)=0$ gives us $\\boxed{w=\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}}\\;\\;(14)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{b}}L(w,b,\\xi,\\alpha,\\beta)=-\\sum_{i=1}^{N}\\alpha_{i}y_{i}$\n",
    "\n",
    ">Setting $\\frac{\\partial}{\\partial{b}}L(w,b,\\xi,\\alpha,\\beta)=0$ gives us $\\boxed{\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0}\\;\\;(15)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{\\xi}}L(w,b,\\xi,\\alpha,\\beta)=C-\\alpha_{i}-\\beta_{i}$\n",
    "\n",
    ">$\\begin{align*}\n",
    "\\text{Setting }&\\frac{\\partial}{\\partial{\\xi}}L(w,b,\\xi,\\alpha,\\beta)=0\\\\\n",
    "&=>C-\\alpha_{i}-\\beta_{i}=0\\\\\n",
    "&=>\\boxed{\\beta_{i}=C-\\alpha_{i}}\\;\\;(16)\\\\\\\\\n",
    "&\\text{But because the }\\beta_{i}\\text{'s are a dual variables with }\\beta_{i}\\geq0\\text{, then this leads to:}\\\\\n",
    "&=>C-\\alpha_{i}\\geq0\\\\\n",
    "&=>\\alpha_{i}\\leq C\\\\\\\\\n",
    "&\\text{This along with the fact that }\\alpha_{i}\\text{ are dual variables with }\\alpha_{i}\\geq0\\text{ we have:}\\\\\n",
    "&\\boxed{0\\leq\\alpha_{i}\\leq C}\\;\\;(17)\n",
    "\\end{align*}$\n",
    "\n",
    "We now will take these results and plug them back into our full lagrangian to get a reduced lagrangian that depends only on $\\alpha$ and $\\beta$:\n",
    "\n",
    "Replacing $w$ using Eq.(14) in Eq.(13), we get\n",
    "\n",
    "$\\begin{align*}\n",
    "L(w,b,\\xi,\\alpha,\\beta)&=\\frac{1}{2}(\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})^{T}(\\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}[(1-\\xi_{i})-y_{i}((\\sum_{j=1}^{N}\\alpha_{j}y_{j}x_{j})^{T}x_{i}+b)]-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}[(1-\\xi_{i})-(\\sum_{j=1}^{N}\\alpha_{j}y_{i}y_{j}x_{j})^{T}x_{i}+b]-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}\\xi_{i}-\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}+b\\sum_{i=1}^{N}\\alpha_{i}y_{i}-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-\\sum_{i=1}^{N}\\alpha_{i}\\xi_{i}+b\\sum_{i=1}^{N}\\alpha_{i}y_{i}-\\sum_{i=1}^{N}\\beta_{i}\\xi_{i}\\\\\n",
    "&=-\\frac{1}{2}\n",
    "\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-(\\sum_{i=1}^{N}\\alpha_{i}+\\sum_{i=1}^{N}\\beta_{i})\\xi_{i}\\;\\;\\;\\;\\text{Using Eq.(15)}\\\\\n",
    "&=-\\frac{1}{2}\n",
    "\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})+C\\sum_{i=1}^{N}\\xi_{i}+\\sum_{i=1}^{N}\\alpha_{i}-C\\sum_{i=1}^{N}\\xi_{i}\\;\\;\\;\\;\\text{Using Eq.(16)}\\\\\n",
    "&=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\n",
    "\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j})\n",
    "\\end{align*}$\n",
    "\n",
    "**Above equation is free of any $w,b,\\xi$ and $\\beta$. Now, let's maximize the $L(w,b,\\xi,\\alpha,\\beta)$ w.r.t. $\\alpha$:**\n",
    "\n",
    "$$\\boxed{\\max_{\\alpha} L(w,b,\\alpha)=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\boxed{\\alpha_{i}\\alpha_{j}}y_{i}y_{j}x_{i}^{T}x_{j})\\\\\n",
    "\\text{s.t. }0\\leq\\alpha_{i}\\leq C\\;\\;\\forall{i}\\\\\n",
    "\\text{and }\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\\;\\;\\forall{i}}\\;\\;(18)$$\n",
    "\n",
    "#### KKT Conditions\n",
    "\n",
    "Eq.(18) is a Quadratic constraint optimization problem because two unknowns $\\alpha_j$ & $\\alpha_j$ are getting multiplied together. **So, this equation is solved using some QP solver (like CVXOPT in python).**\n",
    "\n",
    "Let $\\alpha^{*}_{1},\\alpha^{*}_{2},...\\alpha^{*}_{N}$ be the solutions of QP (Quadratic Programming) problem.\n",
    "\n",
    "**Stationary Condition:**\n",
    "1. $\\frac{\\partial}{\\partial{w}}L(w,b,\\xi,\\alpha,\\beta)=0,\\;\\;\\;\\;$we got Eq.(15)\n",
    "2. $\\frac{\\partial}{\\partial{b}}L(w,b,\\xi,\\alpha,\\beta)=0,\\;\\;\\;\\;$we got Eq.(16)\n",
    "3. $\\frac{\\partial}{\\partial{\\xi}}L(w,b,\\xi,\\alpha,\\beta)=0,\\;\\;\\;\\;$we got Eq.(17)\n",
    "\n",
    "**Primal feasibility condition:**\n",
    "4. $y_{i}(w^{T}x_{i}+b)-(1-\\xi_{i})\\geq 0\\;\\;\\;$\n",
    "5. $\\xi_{i}\\geq0$\n",
    "\n",
    "**Dual feasibility condition:**\n",
    "6. $\\alpha_{i}\\geq0$\n",
    "7. $\\beta_{i}\\geq0$\n",
    "\n",
    "**Complementary slackness condition:**\n",
    "8. $\\alpha_{i}(y_{i}(w^TX_{i}+b)-(1-\\xi_{i}))=0$\n",
    "9. $\\beta{i}(\\xi_{i})=(C-\\alpha_{i})(\\xi_{i})=0\\;\\;$ Using Eq.(16)\n",
    "\n",
    "Using KKT condition 8 & 9, we can imply that:\n",
    "\n",
    "- If $\\alpha^{*}_{i}=0$ then $\\xi_{i}=0$ which implies that $y_{i}(w^TX_{i}+b)\\geq1$\n",
    "- If $0<\\alpha^{*}_{i}<C$ then $\\xi_{i}=0$ which implies that $y_{i}(w^TX_{i}+b)=1$ (**$x_{i}$ is Unbounded Support Vector**)\n",
    "- If $\\alpha^{*}_{i}=C$ then $\\xi_{i}\\geq0$ which implies that $y_{i}(w^TX_{i}+b)=1-\\xi_{i}$\n",
    ">- For $0\\leq\\xi_{i}<1$ $x_{i}$ is correctly classified and lies inside the margin i.e $0<y_{i}(w^TX_{i}+b)\\leq1$ (**$x_{i}$ is Bounded Support Vector**)\n",
    ">- For $\\xi_{i}\\geq1$, $x_{i}$ is misclassified i.e. $y_{i}(w^TX_{i}+b)\\leq0$\n",
    "\n",
    "$$\\boxed{w=\\sum_{i=1}^{N}\\alpha^{*}_{i}y_{i}x_{i}}\\;\\;(19)$$ \n",
    "\n",
    "For $0<\\alpha^{*}_{i}<C$,  \n",
    "$y_{i}(w^TX_{i}+b)-1=0\\;\\;$\n",
    "\n",
    "$$\\boxed{b=\\frac{1}{y_{i}}-w^{T}x_{i}}\\;\\;(20)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Kernel Trick\n",
    "\n",
    "<img src=\"https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2017/02/kernel.png\" width=\"600\">\n",
    "\n",
    "The idea is that our data, which isn't linearly separable in our 'n' dimensional space  **may be linearly separable in a higher dimensional space.** To reach the solution, we sovle the following:\n",
    "\n",
    "$$\\boxed{\\max_{\\alpha} L(w,b,\\alpha)=\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}(\\alpha_{i}\\alpha_{j}y_{i}y_{j}\\boxed{x_{i}^{T}x_{j}})\\\\\n",
    "\\text{s.t. }0\\leq\\alpha_{i}\\leq C\\;\\;\\forall{i}\\\\\n",
    "\\text{and }\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\\;\\;\\forall{i}}$$\n",
    "\n",
    "$XX^{T}??$\n",
    "$$\n",
    "XX^{T}=\\begin{bmatrix}\n",
    "<x_1,x_1>&<x_1,x_2>&...&<x_1,x_m>\\\\\n",
    "<x_2,x_1>&<x_2,x_2>&...&<x_2,x_m>\\\\\n",
    ".&.&...&.\\\\\n",
    ".&.&...&.\\\\\n",
    ".&.&...&.\\\\\n",
    "<x_m,x_1>&<x_m,x_2>&...&<x_m,x_m>\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### The Great Kernel Trick\n",
    "- Replace the dot product $<x_i,x_j>$ with a similarity (Kernel) function $k(x_i,x_j)$.\n",
    "- Replace $XX^{T}$ with $K$ (Gram Matrix)\n",
    "$$K[i][j]=k(x_i,x_j)$$\n",
    "\n",
    "$$\n",
    "K=\\begin{bmatrix}\n",
    "k(x_1,x_1)&k(x_1,x_2)&...&k(x_1,x_m)\\\\\n",
    "k(x_2,x_1)&k(x_2,x_2)&...&k(x_2,x_m)\\\\\n",
    ".&.&...&.\\\\\n",
    ".&.&...&.\\\\\n",
    ".&.&...&.\\\\\n",
    "k(x_m,x_1)&k(x_m,x_2)&...&k(x_m,x_m)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Popular Kernels\n",
    "- Linear Kernel \n",
    "$$k(x_i,x_j)=x_i^Tx_j$$\n",
    "- Polynomial Kernel\n",
    "$$k(x_i,x_j)=(1+x_i^Tx_j)^{d}$$\n",
    "- Radial Basis (RBF) or Gaussian Kernel\n",
    "$$k(x_i,x_j)=exp(\\frac{-1}{2\\sigma^2}||x_i-x_j||^2)$$\n",
    "\n",
    "***Kernels (like RBF) maps our data into higher dimensional space, where its becomes easy to separate the data using optimal hyperplane.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernels:\n",
    "    \n",
    "    @classmethod\n",
    "    def linear(cls,x1,x2):\n",
    "        k=np.dot(x1,x2)\n",
    "        return k\n",
    "    \n",
    "    @classmethod\n",
    "    def polynomial(cls,x1,x2,p=3):\n",
    "        k=np.power((1+np.dot(x1,x2)),p)\n",
    "        return k\n",
    "    \n",
    "    @classmethod\n",
    "    def gaussian(cls,x1,x2,sigma=5.0):\n",
    "        '''Radial basis function'''\n",
    "        k=np.exp(np.power((-np.linalg.norm(x1-x2),2))/(2*np.power(sigma,2)))\n",
    "        return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Implementation using CVXOPT Solver\n",
    "\n",
    "#### Disclaimer\n",
    "This SVM implementation using CVXOPT is only for educational purpose and will not help you win any MNIST competitions. The sole purpose of this implementation is to apply the mathematics explained above and understand the effect of various kernels.\n",
    "\n",
    "#### Rewriting the SVM Optimzation problem in CVXOPT format\n",
    "Since we will solve the optimization problem using CVXOPT library in python, we will need to match the solver's API which, according to the documentation is of the form:\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{x} \\frac{1}{2}x^{T}Px+q^{T}x\\\\\n",
    "s.t.\\;\\;Gx\\leq h\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;Ax=b\n",
    "}$$\n",
    "\n",
    "With api:  \n",
    ">`cvxopt.solvers.qp(P, q[, G, h[, A, b[, solver[, initvals]]]])`\n",
    "\n",
    "Let H be a matrix such that $\\boxed{H_{i,j}=\\sum_{i=1}^{N}\\sum_{j=1}^{N}y_{i}y_{j}(x_{i}x_{j})}$.\n",
    "\n",
    "#### Case 1: Hard Margin SVM\n",
    "\n",
    ">Eq.(9) becomes:\n",
    ">$$\\begin{align*}\n",
    "&\\max_{\\alpha}\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\alpha^{T}H\\alpha\\\\\n",
    "&s.t.\\;\\;\\alpha_{i}\\geq0\\\\\n",
    "&\\;\\;\\;\\;\\;\\;\\;\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\n",
    "\\end{align*}$$\n",
    ">\n",
    ">We convert the sums into vector form and multiply both the objective and the constraint by -1 which turns this into a minimization problem and reverses the inequality :\n",
    ">$$\\boxed{\n",
    "\\min_{\\alpha}\\frac{1}{2}\\alpha^{T}H\\alpha-1^{T}\\alpha\\\\\n",
    "\\text{s.t.}\\;\\;-\\alpha\\leq0\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;y^{T}\\alpha=0\n",
    "}$$\n",
    "\n",
    "#### Case 2: Soft Margin SVM\n",
    "\n",
    ">Eq.(18) becomes:\n",
    ">$$\\begin{align*}\n",
    "&\\max_{\\alpha}\\sum_{i=1}^{N}\\alpha_{i}-\\frac{1}{2}\\alpha^{T}H\\alpha\\\\\n",
    "&s.t.\\;\\;0\\leq \\alpha_{i}\\leq C\\\\\n",
    "&\\;\\;\\;\\;\\;\\;\\;\\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\n",
    "\\end{align*}$$\n",
    ">\n",
    ">We convert the sums into vector form and multiply both the objective and the constraint by -1 which turns this into a minimization problem and reverses the inequality :\n",
    ">$$\\boxed{\n",
    "\\min_{\\alpha}\\frac{1}{2}\\alpha^{T}H\\alpha-1^{T}\\alpha\\\\\n",
    "\\text{s.t.}\\;\\;-\\alpha\\leq 0\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\alpha\\leq C\\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;y^{T}\\alpha=0\n",
    "}$$\n",
    "\n",
    "We are now ready to convert our numpy arrays into the CVXOPT format, using the same notation as in the documentation this gives:\n",
    "- P $:=$ H (a matrix of size $m$ x $m$)\n",
    "- q $:=$ $\\vec{-1}$ (a vector of size $m$ x $1$)  \n",
    "- A $:=$ $y^{T}$ (the label vector of size $1$ x $m$)\n",
    "- b $:=$ 0 (a scalar)\n",
    "\n",
    "**For Hard Margin SVM :**\n",
    "- G $:=$ diag(-1) (a diagonal matrix of -1s of size $m$ x $m$)\n",
    "- h $:=$ $\\vec{0}$ (a vector of zeros of size $m$ x $1$)\n",
    "- For m=2:  \n",
    "$\n",
    "G=\\begin{bmatrix}\n",
    "-1&0\\\\\n",
    "0&-1\\\\\n",
    "\\end{bmatrix},\n",
    "h=\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**For Soft Margin SVM :**\n",
    "- G $:=$ vertical stack of diag(-1) (a diagonal matrix of -1s of size $m$ x $m$) and diag(1) (a diagonal matrix of 1s of size $m$ x $m$)\n",
    "- h $:=$ vertical stack of $\\vec{0}$ (a vector of zeros of size $m$ x $1$) and a vector of ones multiplied by C (size $m$ x $1$)\n",
    "- For m=2:  \n",
    "$\n",
    "G=\\begin{bmatrix}\n",
    "-1&0\\\\\n",
    "0&-1\\\\\n",
    "1&0\\\\\n",
    "0&1\\\\\n",
    "\\end{bmatrix},\n",
    "h=\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "C\\\\\n",
    "C\\\\\n",
    "\\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "\n",
    "    def __init__(self,kernel=Kernels.linear,C=None):\n",
    "        self.kernel=kernel\n",
    "        self.C=C\n",
    "        if self.C is not None:\n",
    "            self.C=float(C)\n",
    "            \n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        # Gram Matrix\n",
    "        K=np.zeros((m,m))\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                K[i,j]=self.kernel(X[i],X[j])\n",
    "        H=np.multiply(np.outer(y,y),K)\n",
    "        P=cvxopt.matrix(H)\n",
    "        q=cvxopt.matrix(np.ones((m,1))*(-1))\n",
    "        A=cvxopt.matrix(y.reshape(1,m).astype(float))\n",
    "        b=cvxopt.matrix(0.0)\n",
    "        # Hard Margin SVM\n",
    "        if self.C==None:\n",
    "            G=cvxopt.matrix(np.identity(m)*(-1))\n",
    "            h=cvxopt.matrix(np.zeros((m,1)))\n",
    "        # Soft Margin SVM\n",
    "        else:\n",
    "            G=cvxopt.matrix(np.row_stack((np.identity(m)*(-1),np.identity(m))))\n",
    "            h=cvxopt.matrix(np.row_stack((np.zeros((m,1)),np.ones((m,1))*self.C)))\n",
    "        # Solve QP Problem\n",
    "        cvxopt.solvers.options['show_progress']=False\n",
    "        solution=cvxopt.solvers.qp(P,q,G,h,A,b)\n",
    "        # Lagrange Mutltpliers\n",
    "        alphas=np.ravel(solution['x'])\n",
    "        # Support Vectors have non zero lagrange multipliers\n",
    "        sv=alphas>1e-5\n",
    "        self.alphas=alphas[sv]\n",
    "        self.svX=X[sv]\n",
    "        self.svY=y[sv]\n",
    "        print(f'Out of {m} training samples, {len(sv[sv==True])} are support vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X,y=make_blobs(n_samples=500,n_features=2,centers=2)\n",
    "y[y==0]=-1\n",
    "data=pd.DataFrame({\n",
    "    'x1':X[:,0],\n",
    "    'x2':X[:,1],\n",
    "    'y':y\n",
    "})\n",
    "sns.scatterplot(data[data['y']==1]['x1'],data[data['y']==1]['x2'])\n",
    "sns.scatterplot(data[data['y']==-1]['x1'],data[data['y']==-1]['x2']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 500 training samples, 2 are support vectors.\n"
     ]
    }
   ],
   "source": [
    "svm=SVM(C=0.5)\n",
    "alphas=svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1&nbsp;&nbsp;&nbsp;&nbsp;Support Vector Machines\n",
    "\n",
    "In the first half of the exercise, we will be using SVMs with various examples 2D datasets. And in the next half of the exercise, we will be using SVM to build a spam classifier.\n",
    "\n",
    "#### 1.1&nbsp;&nbsp;&nbsp;&nbsp;Example Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lagrange Multipliers\n",
    "Lagrange multipliers is a strategy of finding the local maxima and minima of a function subject to **equality** constraints. Let's try to solve a constrained opitimization problem :\n",
    "\n",
    "#### Example 1 (Equality Constraint)\n",
    "\n",
    ">minimize $\\;\\;f(x,y)=2-x^2-2y^2$  \n",
    ">subject to $\\;\\;h(x,y)=x+y-1=0$\n",
    ">\n",
    ">**We introduce a new variable ($\\beta$) called a Lagrange multiplier and study the Lagrange function defined by:**\n",
    ">\n",
    ">$$\\boxed{L(x,y,\\beta)=f(x,y)-\\beta h(x,y)}$$\n",
    ">\n",
    ">$L(x,y,\\beta)=(2-x^2-2y^2)-\\beta(x+y-1)$\n",
    ">\n",
    ">Now we solve the above equation like an unconstrained optimization problem by taking partial derivatives w.r.t $x$ & $y$ and set them equal to zero solving for $x$, $y$ and $\\beta$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{x}}=0\\;\\;=>\\;\\;-2x-\\beta=0\\;\\;=>\\;\\;x=\\frac{-\\beta}{2}$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{y}}=0\\;\\;=>\\;\\;-4y-\\beta=0\\;\\;=>\\;\\;y=\\frac{-\\beta}{4}$\n",
    ">\n",
    ">$\\frac{\\partial{L}}{\\partial{\\beta}}=0\\;\\;=>\\;\\;x+y-1=0\\;\\;=>\\;\\;\\beta=\\frac{-4}{3}$\n",
    ">\n",
    ">$\\boxed{x=\\frac{4}{6},y=\\frac{4}{12},\\beta=\\frac{-4}{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 (Inequality Constraints / Karush-Kuhn-Tucker (KKT) conditions)\n",
    "\n",
    ">maximize $\\;\\;f(x,y)=3x+4y$  \n",
    ">subject to $\\;\\;h_{1}(x,y)=x^2+y^2\\leq4$  \n",
    ">$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{2}(x,y)=x\\geq1$\n",
    ">\n",
    ">**Note: Inequality constraints should be in the form of $h(x,y)\\leq0$**\n",
    ">\n",
    ">$$\\boxed{L(x,y,\\alpha_1,\\alpha_2)=f(x,y)-\\alpha_1 h_{1}(x,y)-\\alpha_2 h_{2}(x,y)\\\\\\;\\;\\text{s.t. }\\alpha_1,\\alpha_2\\geq0}$$\n",
    ">\n",
    ">$L(x,y,\\alpha_1,\\alpha_2)=3x+4y-\\alpha_1(x^2+y^2-4)-\\alpha_2(-x+1)$  \n",
    ">\n",
    ">**KKT Conditions :**\n",
    ">\n",
    ">1. $\\frac{\\partial{L}}{\\partial{x}}=3-2\\alpha_1x+\\alpha_2=0$\n",
    ">\n",
    ">2. $\\frac{\\partial{L}}{\\partial{y}}=4-2\\alpha_1y=0$\n",
    ">\n",
    ">3. $\\alpha_1(x^2+y^2-4)=0$\n",
    ">\n",
    ">4. $\\alpha_2(-x+1)=0$\n",
    ">\n",
    ">5. $\\alpha_1,\\alpha_2\\geq0$ \n",
    ">\n",
    ">A constraint is considered to be binding (active) if changing it also changes the optimal solution. Less severe constraints that do not affect the optimal solution are non-binding (non active). For 2 constraints possible combinations are :\n",
    ">\n",
    ">- Both constraints are binding\n",
    ">- Constraint 1 binding, Constraint 2 not binding\n",
    ">- Constraint 2 binding, Constraing 1 not binding\n",
    ">- Both constraints are not binding\n",
    ">\n",
    ">**POSSIBILITY 1 : Both constraints are binding**\n",
    ">\n",
    ">$-x+1=0\\;\\text{and}\\;\\alpha_2>0\\;\\;=>\\;\\;x=1$  \n",
    ">$x^2+y^2-4=0\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x^2+y^2=4\\;\\;=>\\;\\;1+y^2=4\\;\\;=>\\;\\;y=\\pm\\sqrt{3}$  \n",
    ">\n",
    ">(a) For $y=+\\sqrt{3}$ \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{2}{\\sqrt{3}}>0$  \n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3-\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    ">\n",
    ">(a) For $y=-\\sqrt{3}$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4+2\\sqrt{3}\\alpha_1=0\\;\\;=>\\;\\;\\alpha_1=\\frac{-2}{\\sqrt{3}}<0$ (KKT condition fails)    \n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1+\\alpha_2=0\\;\\;=>\\;\\;3+\\frac{4}{\\sqrt{3}}+\\alpha_2=0\\;\\;=>\\;\\;\\alpha_2=\\frac{-4}{\\sqrt{3}}-3<0$ (KKT condition fails)\n",
    ">\n",
    ">**POSSIBILITY 2 : Constraint 1 binding , Contraint 2 not binding**\n",
    ">\n",
    ">$x>1\\;\\text{and}\\;\\boxed{\\alpha_2=0}$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1>0\\;\\;=>\\;\\;x=+\\sqrt{4-y^{2}}$  \n",
    ">\n",
    ">>Condition 1 becomes:  \n",
    ">>$3-2\\alpha_1x=0\\;\\;=>\\;\\;x=\\frac{3}{2\\alpha_1}\\;\\;=>\\;\\;3-2\\alpha_1\\sqrt{4-y^{2}}=0\\;\\;=>\\;\\;\\alpha_1=\\frac{3}{2\\sqrt{4-y^{2}}}$  \n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4-\\frac{3y}{\\sqrt{4-y^{2}}}=0\\;\\;=>\\;\\;4\\sqrt{4-y^{2}}=3y\\;\\;=>\\;\\;16(4-y^2)=9y^2\\;\\;=>\\;\\;64-16y^2=9y^2\\;\\;=>\\;\\;64=25y^2\\;\\;=>\\;\\;y=\\pm\\frac{8}{5}$\n",
    ">\n",
    ">$\\boxed{\\alpha_1=\\frac{3}{2\\sqrt{4-\\frac{64}{25}}}=\\frac{3}{2(\\frac{6}{5})}=\\frac{5}{4}>0}$  \n",
    ">$x=+\\sqrt{4-y^{2}}\\;\\;=>\\;\\;x=\\frac{6}{5}$\n",
    ">\n",
    ">1 candidate point: $\\boxed{(x,y)=(\\frac{6}{5},\\frac{8}{5})}$\n",
    ">\n",
    ">**POSSIBILITY 3 : Constraint 2 binding , Contraint 1 not binding**\n",
    ">\n",
    ">$x=1\\;\\text{and}\\;\\alpha_2>0$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    ">\n",
    ">**POSSIBILITY 4 : Both constraints are not binding**\n",
    ">\n",
    ">$x>1\\;\\text{and}\\;\\alpha_2=0$  \n",
    ">$x^2+y^2<4\\;\\text{and}\\;\\alpha_1=0$  \n",
    ">\n",
    ">>Condition 2 becomes:  \n",
    ">>$4-2\\alpha_1y=0\\;\\;=>\\;\\;4=0$ (Contradiction, no candidate points)  \n",
    ">\n",
    ">**Check maximality of the candidate point :**\n",
    ">\n",
    ">$f(\\frac{6}{5},\\frac{8}{5})=3(\\frac{6}{4})+4(\\frac{8}{5})=\\frac{18}{5}+\\frac{32}{5}=10$\n",
    ">\n",
    ">Optimal Solution : $\\boxed{x=\\frac{6}{5},y=\\frac{8}{5},\\alpha_1=0,\\alpha_2=\\frac{5}{4}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling both types of Constraints\n",
    "\n",
    "$$\\boxed{\n",
    "\\min_{w}\\;\\;f(w)\\\\\n",
    "\\text{subject to}\\;\\;g_{i}(w)\\leq0\\;\\;\\;i=1,2,...k\\\\\n",
    "\\text{and}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;h_{i}(w)=0\\;\\;\\;i=1,2,...l\\\\\n",
    "}$$\n",
    "\n",
    "**Generalized Lagrangian**  \n",
    "$$\\boxed{\n",
    "L(w,\\alpha,\\beta)=f(w)+\\sum_{i=1}^{k}\\alpha_{i}g_{i}(w)+\\sum_{i=1}^{l}\\beta_{i}h_{i}(w)\\\\\n",
    "\\text{subject to}\\;\\;\\alpha_{i}\\geq0,\\forall_i\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal and Dual Formulations\n",
    "\n",
    "#### Primal Optimization\n",
    "\n",
    "Let $\\theta_p$ be defined as :\n",
    "$$\\boxed{\\theta_p(w)=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}L(w,\\alpha,\\beta)}$$\n",
    "\n",
    "Original constrained problem is same as :\n",
    "$$\\boxed{p^*=\\min_{w}\\theta_P(w)=\\min_{w}\\max_{\\alpha,\\beta;\\alpha_i\\geq0}L(w,\\alpha,\\beta)}$$\n",
    "\n",
    "Solving $p^*$ is same as solving the constrained optimization problem.\n",
    "\n",
    "$$\\begin{equation}\n",
    "  \\theta_{p}(w)=\\begin{cases}\n",
    "    f(w) & \\text{if all constraints are satifsied}\\\\\n",
    "    \\infty & \\text{else}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual Optimization\n",
    "Let $\\theta_d$ be defined as :\n",
    "$$\\boxed{\\theta_d(w)=\\min_{w}L(w,\\alpha,\\beta)}$$\n",
    "\n",
    "Original constrained problem is same as :\n",
    "$$\\boxed{d^*=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}\\theta_d(w)=\\max_{\\alpha,\\beta;\\alpha_i\\geq0}\\min_{w}L(w,\\alpha,\\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week Duality Theorem (Min-Max Theorem)\n",
    "\n",
    "$$\\boxed{d^{*}\\leq p^{*}}$$\n",
    "\n",
    "Both of them are equal ($d^{*}=p^{*}$) when\n",
    "- f(w) is convex\n",
    "- Constraints are affine (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation between Primal and Dual\n",
    "\n",
    "+ In genral $d^*\\leq p^*$, for SVM optimization the equality holds true.\n",
    "+ Certain conditions should be true.\n",
    "+ Known as the **Kahrun-Kuhn-Tucker (KKT)** conditions.\n",
    "+ For $d^*=p^*=L(w^*,\\alpha^*,\\beta^*)$ :\n",
    ">+ $\\frac{\\partial}{\\partial{w}}L(w^*,\\alpha^*,\\beta^*)=0$   \n",
    ">+ $\\frac{\\partial}{\\partial{\\beta_{i}}}L(w^*,\\alpha^*,\\beta^*)=0\\;\\;\\;\\;\\;\\;\\;i=1,2,...l$  \n",
    ">+ $\\alpha_{i}g_{i}(w^{*})=0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$  \n",
    ">+ $g_{i}(w^{*})\\leq0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$  \n",
    ">+ $\\alpha_{i}\\geq0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;i=1,2,...k$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
